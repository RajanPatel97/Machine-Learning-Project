{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Read in white wine data \n",
    "white = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=';')\n",
    "\n",
    "# Read in red wine data \n",
    "red = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add `type` column to `red` with value 1\n",
    "red['type'] = 1\n",
    "\n",
    "# Add `type` column to `white` with value 0\n",
    "white['type'] = 0\n",
    "\n",
    "# Append `white` to `red`\n",
    "wines = red.append(white, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wines.quality\n",
    "X = wines.drop(['quality', 'residual sugar', 'free sulfur dioxide', 'type'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=46, stratify=y)\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Scale the train set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "import keras.backend as K\n",
    "from sklearn import preprocessing, model_selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10, input_dim=9, activation='relu'))\n",
    "    #layer 2\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #layer 3\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [60,80,100,120]\n",
    "epochs = [80,100,120]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.508366 using {'batch_size': 120, 'epochs': 100}\n",
      "0.532900 (0.020510) with: {'batch_size': 60, 'epochs': 80}\n",
      "0.531709 (0.044130) with: {'batch_size': 60, 'epochs': 100}\n",
      "11.852289 (15.982616) with: {'batch_size': 60, 'epochs': 120}\n",
      "0.516958 (0.021361) with: {'batch_size': 80, 'epochs': 80}\n",
      "0.525624 (0.028856) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.554232 (0.016912) with: {'batch_size': 80, 'epochs': 120}\n",
      "0.517747 (0.018006) with: {'batch_size': 100, 'epochs': 80}\n",
      "0.527495 (0.024868) with: {'batch_size': 100, 'epochs': 100}\n",
      "0.532454 (0.022459) with: {'batch_size': 100, 'epochs': 120}\n",
      "11.872272 (16.060993) with: {'batch_size': 120, 'epochs': 80}\n",
      "0.508366 (0.019881) with: {'batch_size': 120, 'epochs': 100}\n",
      "0.519100 (0.018499) with: {'batch_size': 120, 'epochs': 120}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (abs(mean), stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.516263 using {'optimizer': 'Adam'}\n",
      "-0.531237 (0.017942) with: {'optimizer': 'SGD'}\n",
      "-0.560054 (0.003228) with: {'optimizer': 'RMSprop'}\n",
      "-0.516263 (0.023592) with: {'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10, input_dim=9, activation='relu'))\n",
    "    #layer 2\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #layer 3\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=120, verbose=0)\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.512847 using {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "-0.519467 (0.017027) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "-0.532338 (0.024887) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "-0.515076 (0.023720) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "-0.522487 (0.013749) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "-0.529442 (0.023564) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "-0.528949 (0.027145) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "-0.531301 (0.029360) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "-0.525844 (0.015455) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "-11.867671 (16.064245) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "-0.524263 (0.010702) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "-0.527677 (0.010460) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "-0.528387 (0.020540) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "-0.527738 (0.025272) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "-0.527557 (0.021471) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "-0.519742 (0.016295) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "-0.518542 (0.025765) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "-0.517311 (0.022215) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "-0.512914 (0.019999) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "-0.512847 (0.015278) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "-0.524532 (0.020884) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "-11.841568 (15.990190) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "-23.189735 (16.024244) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "-0.523091 (0.023844) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "-0.516556 (0.019353) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "-0.526592 (0.025071) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "-0.532156 (0.036190) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "-0.528229 (0.020145) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "-0.531651 (0.017257) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "-0.517236 (0.020643) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "-11.960967 (16.168796) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "from keras.optimizers import SGD\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10, input_dim=9, activation='relu'))\n",
    "    #layer 2\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #layer 3\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=120, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.536775 using {'activation': 'relu'}\n",
      "-23.984950 (0.130075) with: {'activation': 'softmax'}\n",
      "-0.576420 (0.046004) with: {'activation': 'softplus'}\n",
      "-23.992634 (0.130096) with: {'activation': 'softsign'}\n",
      "-0.536775 (0.046706) with: {'activation': 'relu'}\n",
      "-23.984950 (0.130075) with: {'activation': 'tanh'}\n",
      "-23.984950 (0.130075) with: {'activation': 'sigmoid'}\n",
      "-23.984950 (0.130075) with: {'activation': 'hard_sigmoid'}\n",
      "-0.584799 (0.039769) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the activation function\n",
    "def create_model(activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10, input_dim=9, activation=activation, kernel_initializer='uniform'))\n",
    "    #layer 2\n",
    "    model.add(Dense(100, activation=activation, kernel_initializer='uniform'))\n",
    "    #layer 3\n",
    "    model.add(Dense(50, activation=activation, kernel_initializer='uniform'))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation=activation, kernel_initializer='uniform'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.523963 using {'init_mode': 'normal'}\n",
      "-0.534634 (0.024913) with: {'init_mode': 'uniform'}\n",
      "-0.532389 (0.028179) with: {'init_mode': 'lecun_uniform'}\n",
      "-0.523963 (0.016248) with: {'init_mode': 'normal'}\n",
      "-34.622713 (0.154073) with: {'init_mode': 'zero'}\n",
      "-0.532289 (0.008250) with: {'init_mode': 'glorot_normal'}\n",
      "-0.532570 (0.030123) with: {'init_mode': 'glorot_uniform'}\n",
      "-0.529786 (0.022963) with: {'init_mode': 'he_normal'}\n",
      "-0.552249 (0.012892) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the weight initialization\n",
    "def create_model(init_mode='uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10,  kernel_initializer=init_mode, input_dim=9, activation='relu'))\n",
    "    #layer 2\n",
    "    model.add(Dense(100, kernel_initializer=init_mode, activation='relu'))\n",
    "    #layer 3\n",
    "    model.add(Dense(50,  kernel_initializer=init_mode, activation='relu'))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='relu'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=120, verbose=0)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.504731 using {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "-0.521213 (0.010396) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "-0.544548 (0.040466) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "-0.522952 (0.017470) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "-0.544992 (0.049895) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "-0.516948 (0.015106) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "-0.517371 (0.014801) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "-0.518984 (0.018636) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "-0.508462 (0.013945) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "-0.521706 (0.018597) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "-0.522343 (0.025899) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "-0.517523 (0.025605) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "-0.509635 (0.021341) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "-0.514077 (0.012430) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "-0.518255 (0.009486) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "-0.513829 (0.012324) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "-0.511376 (0.017738) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "-0.513465 (0.023944) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "-0.527046 (0.017855) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "-0.510604 (0.016622) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "-0.511184 (0.014721) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "-0.522518 (0.028336) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "-0.519627 (0.020096) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "-0.523702 (0.023467) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "-0.504731 (0.024616) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "-0.510401 (0.015315) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "-0.516544 (0.026982) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "-0.510457 (0.023591) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "-0.515263 (0.020768) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "-0.517749 (0.018879) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "-0.533685 (0.018052) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "-0.529713 (0.035296) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "-0.530314 (0.016211) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "-0.518750 (0.019770) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "-0.523043 (0.019486) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "-0.519378 (0.020287) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "-0.532078 (0.016520) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "-0.527554 (0.023103) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "-0.526221 (0.020798) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "-0.532478 (0.015477) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "-0.539936 (0.027446) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "-0.561143 (0.029408) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "-0.547397 (0.026794) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "-0.552308 (0.021475) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "-0.560788 (0.031525) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "-0.549139 (0.023066) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "-0.625832 (0.046163) with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "-0.591207 (0.019420) with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "-0.611759 (0.022253) with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "-0.597895 (0.023740) with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "-0.607906 (0.031664) with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the dropout rate\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10, input_dim=9, activation='relu', kernel_initializer='uniform'))\n",
    "    #layer 2\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    #layer 3\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='uniform'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.513729 using {'neurons': 25}\n",
      "-34.622712 (0.154072) with: {'neurons': 1}\n",
      "-0.608257 (0.021744) with: {'neurons': 5}\n",
      "-0.567983 (0.011782) with: {'neurons': 10}\n",
      "-0.545437 (0.021085) with: {'neurons': 15}\n",
      "-0.518311 (0.022430) with: {'neurons': 20}\n",
      "-0.513729 (0.018417) with: {'neurons': 25}\n",
      "-0.515320 (0.023215) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "#Use scikit-learn to grid search the number of neurons\n",
    "def create_model(neurons=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(neurons, input_dim=9, activation='relu', kernel_initializer='uniform'))\n",
    "    #layer 2\n",
    "    model.add(Dense(neurons, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #layer 3\n",
    "    model.add(Dense(neurons, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='uniform'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "import keras.backend as K\n",
    "from sklearn import preprocessing, model_selection \n",
    "\n",
    "\n",
    "def create_model(neurons=1, optimizer='adam', activation = 'relu', hidden_layers=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(10, input_dim=9, activation='relu'))\n",
    "    \n",
    "    #hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "      # Add one hidden layer\n",
    "      model.add(Dense(neurons, activation=activation))\n",
    "    \n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0, epochs=100)\n",
    "# define the grid search parameters\n",
    "hidden_layers = [1,2,3,4]\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "activation = ['relu', 'tanh', 'sigmoid','linear']\n",
    "param_grid = dict(hidden_layers = hidden_layers, neurons=neurons, activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.511264 using {'activation': 'relu', 'hidden_layers': 4, 'neurons': 30}\n",
      "-23.308029 (16.120196) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 1}\n",
      "-0.525755 (0.022679) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 5}\n",
      "-0.525120 (0.017648) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 10}\n",
      "-0.516731 (0.017358) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 15}\n",
      "-0.523429 (0.024166) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 20}\n",
      "-0.515002 (0.017678) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 25}\n",
      "-0.527109 (0.018615) with: {'activation': 'relu', 'hidden_layers': 1, 'neurons': 30}\n",
      "-23.188538 (16.025937) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 1}\n",
      "-0.529002 (0.022905) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 5}\n",
      "-0.526482 (0.015536) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 10}\n",
      "-0.524952 (0.021357) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 15}\n",
      "-11.963575 (16.166954) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 20}\n",
      "-0.529357 (0.030108) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 25}\n",
      "-0.521638 (0.010022) with: {'activation': 'relu', 'hidden_layers': 2, 'neurons': 30}\n",
      "-34.622713 (0.154072) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 1}\n",
      "-23.193487 (16.018937) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 5}\n",
      "-0.524032 (0.020225) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 10}\n",
      "-0.522550 (0.016588) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 15}\n",
      "-11.883928 (16.052757) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 20}\n",
      "-0.539594 (0.006908) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 25}\n",
      "-0.544786 (0.020701) with: {'activation': 'relu', 'hidden_layers': 3, 'neurons': 30}\n",
      "-23.304609 (16.125033) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 1}\n",
      "-11.878391 (16.056667) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 5}\n",
      "-0.526742 (0.024744) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 10}\n",
      "-0.521004 (0.018309) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 15}\n",
      "-0.527410 (0.023122) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 20}\n",
      "-0.550088 (0.030452) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 25}\n",
      "-0.511264 (0.016048) with: {'activation': 'relu', 'hidden_layers': 4, 'neurons': 30}\n",
      "-0.534350 (0.024700) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 1}\n",
      "-0.525168 (0.020010) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 5}\n",
      "-0.515333 (0.019260) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 10}\n",
      "-0.519911 (0.027145) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 15}\n",
      "-0.526636 (0.019735) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 20}\n",
      "-0.514125 (0.020916) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 25}\n",
      "-0.516620 (0.015853) with: {'activation': 'tanh', 'hidden_layers': 1, 'neurons': 30}\n",
      "-0.527349 (0.014669) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 1}\n",
      "-0.523791 (0.018936) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 5}\n",
      "-0.519179 (0.016393) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 10}\n",
      "-0.516895 (0.016032) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 15}\n",
      "-0.519121 (0.017799) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 20}\n",
      "-0.516284 (0.017378) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 25}\n",
      "-0.531197 (0.017541) with: {'activation': 'tanh', 'hidden_layers': 2, 'neurons': 30}\n",
      "-0.527925 (0.020966) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 1}\n",
      "-0.541717 (0.023765) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 5}\n",
      "-0.520901 (0.013628) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 10}\n",
      "-0.517754 (0.018674) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 15}\n",
      "-0.515914 (0.020243) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 20}\n",
      "-0.526493 (0.019390) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 25}\n",
      "-0.525535 (0.023160) with: {'activation': 'tanh', 'hidden_layers': 3, 'neurons': 30}\n",
      "-0.532579 (0.020335) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 1}\n",
      "-0.526385 (0.017271) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 5}\n",
      "-0.523750 (0.015049) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 10}\n",
      "-0.513601 (0.020848) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 15}\n",
      "-0.535555 (0.025830) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 20}\n",
      "-0.538706 (0.017517) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 25}\n",
      "-0.524776 (0.022756) with: {'activation': 'tanh', 'hidden_layers': 4, 'neurons': 30}\n",
      "-23.194563 (16.017415) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 1}\n",
      "-23.303422 (16.126711) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 5}\n",
      "-11.840913 (15.990649) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 10}\n",
      "-23.270634 (16.080982) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 15}\n",
      "-11.868114 (16.063935) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 20}\n",
      "-11.835499 (15.994477) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 25}\n",
      "-11.865120 (16.066050) with: {'activation': 'sigmoid', 'hidden_layers': 1, 'neurons': 30}\n",
      "-0.523884 (0.024847) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 1}\n",
      "-11.874815 (16.059194) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 5}\n",
      "-34.622713 (0.154072) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 10}\n",
      "-0.524884 (0.020345) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 15}\n",
      "-0.531523 (0.027266) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 20}\n",
      "-11.879859 (16.055632) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 25}\n",
      "-23.190941 (16.022539) with: {'activation': 'sigmoid', 'hidden_layers': 2, 'neurons': 30}\n",
      "-23.312520 (16.113844) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 1}\n",
      "-23.275319 (16.074357) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 5}\n",
      "-0.525826 (0.016961) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 10}\n",
      "-0.533775 (0.020488) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 15}\n",
      "-0.526172 (0.015270) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 20}\n",
      "-11.960864 (16.168874) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 25}\n",
      "-11.954381 (16.173462) with: {'activation': 'sigmoid', 'hidden_layers': 3, 'neurons': 30}\n",
      "-11.993514 (16.145805) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 1}\n",
      "-11.966965 (16.164559) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 5}\n",
      "-11.954997 (16.173020) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 10}\n",
      "-23.191682 (16.021490) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 15}\n",
      "-23.273033 (16.077588) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 20}\n",
      "-11.953508 (16.174078) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 25}\n",
      "-23.192035 (16.020990) with: {'activation': 'sigmoid', 'hidden_layers': 4, 'neurons': 30}\n",
      "-0.539798 (0.022951) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 1}\n",
      "-0.527005 (0.016278) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 5}\n",
      "-0.538244 (0.011969) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 10}\n",
      "-0.532748 (0.018909) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 15}\n",
      "-0.536471 (0.019235) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 20}\n",
      "-0.527423 (0.020784) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 25}\n",
      "-0.527249 (0.019225) with: {'activation': 'linear', 'hidden_layers': 1, 'neurons': 30}\n",
      "-0.531050 (0.018264) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 1}\n",
      "-0.536646 (0.027774) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 5}\n",
      "-0.527537 (0.026177) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 10}\n",
      "-0.530653 (0.016574) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 15}\n",
      "-0.530927 (0.010325) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 20}\n",
      "-0.528263 (0.027904) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 25}\n",
      "-0.533193 (0.011117) with: {'activation': 'linear', 'hidden_layers': 2, 'neurons': 30}\n",
      "-0.531427 (0.019433) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 1}\n",
      "-0.528875 (0.009600) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 5}\n",
      "-0.524832 (0.022101) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 10}\n",
      "-0.533897 (0.026989) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 15}\n",
      "-0.524581 (0.017552) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 20}\n",
      "-0.542632 (0.015096) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 25}\n",
      "-0.535977 (0.024512) with: {'activation': 'linear', 'hidden_layers': 3, 'neurons': 30}\n",
      "-0.526731 (0.015966) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 1}\n",
      "-0.538035 (0.026091) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 5}\n",
      "-0.532782 (0.018217) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 10}\n",
      "-0.533624 (0.023286) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 15}\n",
      "-0.538474 (0.007848) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 20}\n",
      "-0.526715 (0.020771) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 25}\n",
      "-0.528903 (0.021057) with: {'activation': 'linear', 'hidden_layers': 4, 'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "import keras.backend as K\n",
    "from sklearn import preprocessing, model_selection \n",
    "\n",
    "\n",
    "def create_model(neurons=1, optimizer='adam', activation = 'relu', hidden_layers=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    model.add(Dense(9, input_dim=9, activation='relu'))\n",
    "    \n",
    "    #hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        # Add one hidden layer\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "    \n",
    "    #layer 4\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0, epochs=100)\n",
    "# define the grid search parameters\n",
    "hidden_layers = [1,2,3,4]\n",
    "neurons = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "activation = ['relu', 'tanh', 'sigmoid','linear']\n",
    "param_grid = dict(hidden_layers = hidden_layers, neurons=neurons, activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
