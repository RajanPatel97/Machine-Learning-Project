{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Read in white wine data \n",
    "white = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=';')\n",
    "\n",
    "# Read in red wine data \n",
    "red = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add `type` column to `red` with value 1\n",
    "red['type'] = 1\n",
    "\n",
    "# Add `type` column to `white` with value 0\n",
    "white['type'] = 0\n",
    "\n",
    "# Append `white` to `red`\n",
    "wines = red.append(white, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wines.quality\n",
    "X = wines.drop(['quality', 'residual sugar', 'free sulfur dioxide', 'type'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=46, stratify=y)\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Scale the train set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=200, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpregressor = MLPRegressor(hidden_layer_sizes=(200), max_iter=500, alpha = 0.001)  \n",
    "mlpregressor.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>6</td>\n",
       "      <td>5.911639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>6</td>\n",
       "      <td>6.097940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5040</th>\n",
       "      <td>5</td>\n",
       "      <td>6.215848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>7</td>\n",
       "      <td>6.427831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>7</td>\n",
       "      <td>6.129530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>6</td>\n",
       "      <td>6.353232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>5</td>\n",
       "      <td>5.194208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>6</td>\n",
       "      <td>5.391799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>5</td>\n",
       "      <td>5.282166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6</td>\n",
       "      <td>5.624213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>7</td>\n",
       "      <td>6.861119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>7</td>\n",
       "      <td>5.814858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>5</td>\n",
       "      <td>5.915157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>6</td>\n",
       "      <td>5.809701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>5</td>\n",
       "      <td>5.356301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6402</th>\n",
       "      <td>7</td>\n",
       "      <td>6.238553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>6</td>\n",
       "      <td>6.215741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>6</td>\n",
       "      <td>6.304207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>6</td>\n",
       "      <td>6.249108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>7</td>\n",
       "      <td>6.698387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4</td>\n",
       "      <td>5.567998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>8</td>\n",
       "      <td>6.167997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>6</td>\n",
       "      <td>6.414853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>5</td>\n",
       "      <td>5.320675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5</td>\n",
       "      <td>5.198935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>7</td>\n",
       "      <td>7.107898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>6</td>\n",
       "      <td>6.789814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>6</td>\n",
       "      <td>5.975168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6123</th>\n",
       "      <td>6</td>\n",
       "      <td>5.879433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>7</td>\n",
       "      <td>6.476813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>6</td>\n",
       "      <td>5.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>5</td>\n",
       "      <td>5.630922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>5</td>\n",
       "      <td>5.320336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>6</td>\n",
       "      <td>6.816310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>6</td>\n",
       "      <td>5.434639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>6</td>\n",
       "      <td>6.236113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5</td>\n",
       "      <td>5.152889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>7</td>\n",
       "      <td>6.329580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6197</th>\n",
       "      <td>6</td>\n",
       "      <td>6.030957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>7</td>\n",
       "      <td>5.891319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>6</td>\n",
       "      <td>5.842822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>6</td>\n",
       "      <td>5.886916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>5</td>\n",
       "      <td>5.511749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>6</td>\n",
       "      <td>6.413320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>5</td>\n",
       "      <td>5.291676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>5</td>\n",
       "      <td>4.529835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>6</td>\n",
       "      <td>5.713771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>7</td>\n",
       "      <td>6.077339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6</td>\n",
       "      <td>5.259983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>6</td>\n",
       "      <td>5.465405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>5</td>\n",
       "      <td>5.054113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>6</td>\n",
       "      <td>5.453788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>6</td>\n",
       "      <td>6.348817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>5</td>\n",
       "      <td>5.233881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>6</td>\n",
       "      <td>6.051540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>6</td>\n",
       "      <td>5.904457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>4</td>\n",
       "      <td>5.346079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>5</td>\n",
       "      <td>5.201867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>6</td>\n",
       "      <td>6.063610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>5</td>\n",
       "      <td>6.010210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted\n",
       "1639       6   5.911639\n",
       "5140       6   6.097940\n",
       "5040       5   6.215848\n",
       "3383       7   6.427831\n",
       "5176       7   6.129530\n",
       "4022       6   6.353232\n",
       "2806       5   5.194208\n",
       "1779       6   5.391799\n",
       "1262       5   5.282166\n",
       "116        6   5.624213\n",
       "1845       7   6.861119\n",
       "2519       7   5.814858\n",
       "1247       5   5.915157\n",
       "5800       6   5.809701\n",
       "417        5   5.356301\n",
       "6402       7   6.238553\n",
       "1490       6   6.215741\n",
       "4813       6   6.304207\n",
       "5149       6   6.249108\n",
       "4921       7   6.698387\n",
       "4092       4   5.567998\n",
       "5088       8   6.167997\n",
       "1126       6   6.414853\n",
       "848        5   5.320675\n",
       "2001       5   5.198935\n",
       "5875       7   7.107898\n",
       "2003       6   6.789814\n",
       "1390       6   5.975168\n",
       "6123       6   5.879433\n",
       "4256       7   6.476813\n",
       "...      ...        ...\n",
       "2293       6   5.533838\n",
       "5955       5   5.630922\n",
       "1241       5   5.320336\n",
       "1172       6   6.816310\n",
       "2380       6   5.434639\n",
       "5305       6   6.236113\n",
       "183        5   5.152889\n",
       "514        7   6.329580\n",
       "6197       6   6.030957\n",
       "5217       7   5.891319\n",
       "4571       6   5.842822\n",
       "4056       6   5.886916\n",
       "611        5   5.511749\n",
       "5650       6   6.413320\n",
       "3878       5   5.291676\n",
       "4253       5   4.529835\n",
       "382        6   5.713771\n",
       "2557       7   6.077339\n",
       "91         6   5.259983\n",
       "566        6   5.465405\n",
       "761        5   5.054113\n",
       "426        6   5.453788\n",
       "1580       6   6.348817\n",
       "105        5   5.233881\n",
       "5533       6   6.051540\n",
       "2347       6   5.904457\n",
       "2301       4   5.346079\n",
       "993        5   5.201867\n",
       "466        6   6.063610\n",
       "3064       5   6.010210\n",
       "\n",
       "[650 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlpregressor.predict(X_test_scaled)\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5387717828829957\n",
      "Mean Squared Error: 0.48090349957433126\n",
      "Root Mean Squared Error: 0.6934720611346439\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpregressor.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn import preprocessing, model_selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "winemod1 = Sequential()\n",
    "# layer 1\n",
    "winemod1.add(Dense(9, input_dim=9, activation='relu', name='fc0',kernel_regularizer=regularizers.l2(0.001)))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "#layer 2\n",
    "winemod1.add(Dense(50, name='fc1',bias_initializer='zeros'))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "winemod1.add(Activation('relu'))\n",
    "winemod1.add(Dropout(0.5))\n",
    "#layer 3\n",
    "winemod1.add(Dense(100, name='fc2',bias_initializer='zeros'))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "winemod1.add(Activation('relu'))\n",
    "winemod1.add(Dropout(0.5))\n",
    "#layer 4\n",
    "winemod1.add(Dense(1, name='fc3',bias_initializer='zeros'))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "winemod1.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc0 (Dense)                  (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9)                 36        \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 50)                500       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 6,431\n",
      "Trainable params: 6,111\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "winemod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "Adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "winemod1.compile(optimizer = Adam, loss='mean_squared_error', metrics=[metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5847 samples, validate on 650 samples\n",
      "Epoch 1/200\n",
      "5847/5847 [==============================] - 1s 216us/step - loss: 34.4980 - mean_absolute_error: 5.7738 - categorical_accuracy: 1.0000 - val_loss: 32.1556 - val_mean_absolute_error: 5.5872 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 33.2088 - mean_absolute_error: 5.6825 - categorical_accuracy: 1.0000 - val_loss: 31.5803 - val_mean_absolute_error: 5.5496 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 32.0767 - mean_absolute_error: 5.5917 - categorical_accuracy: 1.0000 - val_loss: 30.9824 - val_mean_absolute_error: 5.5070 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 30.9963 - mean_absolute_error: 5.5016 - categorical_accuracy: 1.0000 - val_loss: 29.9570 - val_mean_absolute_error: 5.4188 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 29.9682 - mean_absolute_error: 5.4122 - categorical_accuracy: 1.0000 - val_loss: 28.9160 - val_mean_absolute_error: 5.3252 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 28.9861 - mean_absolute_error: 5.3235 - categorical_accuracy: 1.0000 - val_loss: 27.9244 - val_mean_absolute_error: 5.2328 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 28.0265 - mean_absolute_error: 5.2355 - categorical_accuracy: 1.0000 - val_loss: 27.0040 - val_mean_absolute_error: 5.1452 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 27.1057 - mean_absolute_error: 5.1481 - categorical_accuracy: 1.0000 - val_loss: 26.3915 - val_mean_absolute_error: 5.0859 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 26.2051 - mean_absolute_error: 5.0613 - categorical_accuracy: 1.0000 - val_loss: 25.3924 - val_mean_absolute_error: 4.9871 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 25.3424 - mean_absolute_error: 4.9751 - categorical_accuracy: 1.0000 - val_loss: 24.6659 - val_mean_absolute_error: 4.9139 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 24.4859 - mean_absolute_error: 4.8895 - categorical_accuracy: 1.0000 - val_loss: 23.7848 - val_mean_absolute_error: 4.8234 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "5847/5847 [==============================] - ETA: 0s - loss: 23.6515 - mean_absolute_error: 4.8034 - categorical_accuracy: 1.000 - 0s 79us/step - loss: 23.6605 - mean_absolute_error: 4.8045 - categorical_accuracy: 1.0000 - val_loss: 22.9484 - val_mean_absolute_error: 4.7359 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 22.8479 - mean_absolute_error: 4.7198 - categorical_accuracy: 1.0000 - val_loss: 22.2132 - val_mean_absolute_error: 4.6578 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 22.0601 - mean_absolute_error: 4.6360 - categorical_accuracy: 1.0000 - val_loss: 21.3673 - val_mean_absolute_error: 4.5661 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 21.2961 - mean_absolute_error: 4.5528 - categorical_accuracy: 1.0000 - val_loss: 20.6316 - val_mean_absolute_error: 4.4849 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 20.5476 - mean_absolute_error: 4.4699 - categorical_accuracy: 1.0000 - val_loss: 19.9362 - val_mean_absolute_error: 4.4067 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 19.8141 - mean_absolute_error: 4.3876 - categorical_accuracy: 1.0000 - val_loss: 19.3364 - val_mean_absolute_error: 4.3384 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "5847/5847 [==============================] - 0s 83us/step - loss: 19.1008 - mean_absolute_error: 4.3059 - categorical_accuracy: 1.0000 - val_loss: 18.6459 - val_mean_absolute_error: 4.2581 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 18.4057 - mean_absolute_error: 4.2248 - categorical_accuracy: 1.0000 - val_loss: 17.9284 - val_mean_absolute_error: 4.1731 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 17.7278 - mean_absolute_error: 4.1440 - categorical_accuracy: 1.0000 - val_loss: 17.1884 - val_mean_absolute_error: 4.0837 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 17.0712 - mean_absolute_error: 4.0638 - categorical_accuracy: 1.0000 - val_loss: 16.5603 - val_mean_absolute_error: 4.0059 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "5847/5847 [==============================] - 0s 75us/step - loss: 16.4313 - mean_absolute_error: 3.9842 - categorical_accuracy: 1.0000 - val_loss: 15.9222 - val_mean_absolute_error: 3.9256 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 15.7995 - mean_absolute_error: 3.9049 - categorical_accuracy: 1.0000 - val_loss: 15.3341 - val_mean_absolute_error: 3.8500 - val_categorical_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 15.1931 - mean_absolute_error: 3.8262 - categorical_accuracy: 1.0000 - val_loss: 14.7605 - val_mean_absolute_error: 3.7749 - val_categorical_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "5847/5847 [==============================] - 0s 80us/step - loss: 14.6009 - mean_absolute_error: 3.7481 - categorical_accuracy: 1.0000 - val_loss: 14.2133 - val_mean_absolute_error: 3.7020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 14.0202 - mean_absolute_error: 3.6703 - categorical_accuracy: 1.0000 - val_loss: 13.6633 - val_mean_absolute_error: 3.6270 - val_categorical_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 13.4549 - mean_absolute_error: 3.5928 - categorical_accuracy: 1.0000 - val_loss: 13.1008 - val_mean_absolute_error: 3.5489 - val_categorical_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "5847/5847 [==============================] - 0s 75us/step - loss: 12.9098 - mean_absolute_error: 3.5161 - categorical_accuracy: 1.0000 - val_loss: 12.5090 - val_mean_absolute_error: 3.4648 - val_categorical_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 12.3761 - mean_absolute_error: 3.4399 - categorical_accuracy: 1.0000 - val_loss: 11.9523 - val_mean_absolute_error: 3.3835 - val_categorical_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 11.8599 - mean_absolute_error: 3.3641 - categorical_accuracy: 1.0000 - val_loss: 11.5273 - val_mean_absolute_error: 3.3206 - val_categorical_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 11.3609 - mean_absolute_error: 3.2885 - categorical_accuracy: 1.0000 - val_loss: 10.9287 - val_mean_absolute_error: 3.2293 - val_categorical_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 10.8687 - mean_absolute_error: 3.2141 - categorical_accuracy: 1.0000 - val_loss: 10.5609 - val_mean_absolute_error: 3.1719 - val_categorical_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 10.3997 - mean_absolute_error: 3.1397 - categorical_accuracy: 1.0000 - val_loss: 10.0635 - val_mean_absolute_error: 3.0925 - val_categorical_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 9.9406 - mean_absolute_error: 3.0658 - categorical_accuracy: 1.0000 - val_loss: 9.6110 - val_mean_absolute_error: 3.0183 - val_categorical_accuracy: 1.0000\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5847/5847 [==============================] - 0s 69us/step - loss: 9.4945 - mean_absolute_error: 2.9925 - categorical_accuracy: 1.0000 - val_loss: 9.1193 - val_mean_absolute_error: 2.9355 - val_categorical_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 9.0590 - mean_absolute_error: 2.9199 - categorical_accuracy: 1.0000 - val_loss: 8.7318 - val_mean_absolute_error: 2.8692 - val_categorical_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 8.6401 - mean_absolute_error: 2.8475 - categorical_accuracy: 1.0000 - val_loss: 8.3048 - val_mean_absolute_error: 2.7940 - val_categorical_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 8.2371 - mean_absolute_error: 2.7759 - categorical_accuracy: 1.0000 - val_loss: 7.8861 - val_mean_absolute_error: 2.7179 - val_categorical_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 7.8439 - mean_absolute_error: 2.7043 - categorical_accuracy: 1.0000 - val_loss: 7.5678 - val_mean_absolute_error: 2.6588 - val_categorical_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 7.4648 - mean_absolute_error: 2.6335 - categorical_accuracy: 1.0000 - val_loss: 7.2751 - val_mean_absolute_error: 2.6036 - val_categorical_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 7.0958 - mean_absolute_error: 2.5634 - categorical_accuracy: 1.0000 - val_loss: 6.9030 - val_mean_absolute_error: 2.5313 - val_categorical_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 6.7423 - mean_absolute_error: 2.4940 - categorical_accuracy: 1.0000 - val_loss: 6.5597 - val_mean_absolute_error: 2.4630 - val_categorical_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 6.4002 - mean_absolute_error: 2.4252 - categorical_accuracy: 1.0000 - val_loss: 6.1890 - val_mean_absolute_error: 2.3873 - val_categorical_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 6.0727 - mean_absolute_error: 2.3568 - categorical_accuracy: 1.0000 - val_loss: 5.8656 - val_mean_absolute_error: 2.3193 - val_categorical_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 5.7557 - mean_absolute_error: 2.2891 - categorical_accuracy: 1.0000 - val_loss: 5.5075 - val_mean_absolute_error: 2.2411 - val_categorical_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 5.4506 - mean_absolute_error: 2.2217 - categorical_accuracy: 1.0000 - val_loss: 5.2526 - val_mean_absolute_error: 2.1837 - val_categorical_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 5.1555 - mean_absolute_error: 2.1553 - categorical_accuracy: 1.0000 - val_loss: 4.9145 - val_mean_absolute_error: 2.1054 - val_categorical_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 4.8678 - mean_absolute_error: 2.0900 - categorical_accuracy: 1.0000 - val_loss: 4.7281 - val_mean_absolute_error: 2.0616 - val_categorical_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 4.6092 - mean_absolute_error: 2.0256 - categorical_accuracy: 1.0000 - val_loss: 4.4648 - val_mean_absolute_error: 1.9972 - val_categorical_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 4.3435 - mean_absolute_error: 1.9611 - categorical_accuracy: 1.0000 - val_loss: 4.2067 - val_mean_absolute_error: 1.9326 - val_categorical_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 4.0952 - mean_absolute_error: 1.8972 - categorical_accuracy: 1.0000 - val_loss: 3.9378 - val_mean_absolute_error: 1.8632 - val_categorical_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 3.8559 - mean_absolute_error: 1.8349 - categorical_accuracy: 1.0000 - val_loss: 3.7022 - val_mean_absolute_error: 1.7998 - val_categorical_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 3.6239 - mean_absolute_error: 1.7728 - categorical_accuracy: 1.0000 - val_loss: 3.4602 - val_mean_absolute_error: 1.7323 - val_categorical_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 3.4082 - mean_absolute_error: 1.7115 - categorical_accuracy: 1.0000 - val_loss: 3.2517 - val_mean_absolute_error: 1.6726 - val_categorical_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 3.2049 - mean_absolute_error: 1.6538 - categorical_accuracy: 1.0000 - val_loss: 3.0351 - val_mean_absolute_error: 1.6071 - val_categorical_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 3.0091 - mean_absolute_error: 1.5941 - categorical_accuracy: 1.0000 - val_loss: 2.8558 - val_mean_absolute_error: 1.5531 - val_categorical_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 2.8195 - mean_absolute_error: 1.5350 - categorical_accuracy: 1.0000 - val_loss: 2.6852 - val_mean_absolute_error: 1.4978 - val_categorical_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 2.6396 - mean_absolute_error: 1.4776 - categorical_accuracy: 1.0000 - val_loss: 2.5106 - val_mean_absolute_error: 1.4408 - val_categorical_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 2.4702 - mean_absolute_error: 1.4221 - categorical_accuracy: 1.0000 - val_loss: 2.3463 - val_mean_absolute_error: 1.3837 - val_categorical_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 2.3167 - mean_absolute_error: 1.3678 - categorical_accuracy: 1.0000 - val_loss: 2.2201 - val_mean_absolute_error: 1.3405 - val_categorical_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 2.1679 - mean_absolute_error: 1.3143 - categorical_accuracy: 1.0000 - val_loss: 2.0322 - val_mean_absolute_error: 1.2719 - val_categorical_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 2.0213 - mean_absolute_error: 1.2629 - categorical_accuracy: 1.0000 - val_loss: 1.9560 - val_mean_absolute_error: 1.2432 - val_categorical_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 1.8895 - mean_absolute_error: 1.2126 - categorical_accuracy: 1.0000 - val_loss: 1.7724 - val_mean_absolute_error: 1.1723 - val_categorical_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 1.7674 - mean_absolute_error: 1.1641 - categorical_accuracy: 1.0000 - val_loss: 1.6733 - val_mean_absolute_error: 1.1347 - val_categorical_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 1.6482 - mean_absolute_error: 1.1164 - categorical_accuracy: 1.0000 - val_loss: 1.5730 - val_mean_absolute_error: 1.0918 - val_categorical_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 1.5383 - mean_absolute_error: 1.0700 - categorical_accuracy: 1.0000 - val_loss: 1.4380 - val_mean_absolute_error: 1.0341 - val_categorical_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 1.4419 - mean_absolute_error: 1.0263 - categorical_accuracy: 1.0000 - val_loss: 1.3594 - val_mean_absolute_error: 0.9987 - val_categorical_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 1.3390 - mean_absolute_error: 0.9821 - categorical_accuracy: 1.0000 - val_loss: 1.2630 - val_mean_absolute_error: 0.9546 - val_categorical_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 1.2542 - mean_absolute_error: 0.9428 - categorical_accuracy: 1.0000 - val_loss: 1.1547 - val_mean_absolute_error: 0.9018 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 1.1817 - mean_absolute_error: 0.9067 - categorical_accuracy: 1.0000 - val_loss: 1.0783 - val_mean_absolute_error: 0.8627 - val_categorical_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 1.1056 - mean_absolute_error: 0.8698 - categorical_accuracy: 1.0000 - val_loss: 1.0229 - val_mean_absolute_error: 0.8339 - val_categorical_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 1.0329 - mean_absolute_error: 0.8336 - categorical_accuracy: 1.0000 - val_loss: 0.9707 - val_mean_absolute_error: 0.8052 - val_categorical_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 0.9695 - mean_absolute_error: 0.7993 - categorical_accuracy: 1.0000 - val_loss: 0.8948 - val_mean_absolute_error: 0.7626 - val_categorical_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.9159 - mean_absolute_error: 0.7683 - categorical_accuracy: 1.0000 - val_loss: 0.8534 - val_mean_absolute_error: 0.7386 - val_categorical_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.8634 - mean_absolute_error: 0.7396 - categorical_accuracy: 1.0000 - val_loss: 0.8059 - val_mean_absolute_error: 0.7098 - val_categorical_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.8146 - mean_absolute_error: 0.7131 - categorical_accuracy: 1.0000 - val_loss: 0.7560 - val_mean_absolute_error: 0.6810 - val_categorical_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.7756 - mean_absolute_error: 0.6900 - categorical_accuracy: 1.0000 - val_loss: 0.7293 - val_mean_absolute_error: 0.6639 - val_categorical_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.7360 - mean_absolute_error: 0.6676 - categorical_accuracy: 1.0000 - val_loss: 0.6817 - val_mean_absolute_error: 0.6356 - val_categorical_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.7049 - mean_absolute_error: 0.6473 - categorical_accuracy: 1.0000 - val_loss: 0.6452 - val_mean_absolute_error: 0.6132 - val_categorical_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 0.6794 - mean_absolute_error: 0.6317 - categorical_accuracy: 1.0000 - val_loss: 0.6374 - val_mean_absolute_error: 0.6090 - val_categorical_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.6537 - mean_absolute_error: 0.6170 - categorical_accuracy: 1.0000 - val_loss: 0.6037 - val_mean_absolute_error: 0.5870 - val_categorical_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.6297 - mean_absolute_error: 0.6051 - categorical_accuracy: 1.0000 - val_loss: 0.5845 - val_mean_absolute_error: 0.5769 - val_categorical_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.6049 - mean_absolute_error: 0.5905 - categorical_accuracy: 1.0000 - val_loss: 0.5538 - val_mean_absolute_error: 0.5596 - val_categorical_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5957 - mean_absolute_error: 0.5851 - categorical_accuracy: 1.0000 - val_loss: 0.5458 - val_mean_absolute_error: 0.5553 - val_categorical_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5741 - mean_absolute_error: 0.5747 - categorical_accuracy: 1.0000 - val_loss: 0.5323 - val_mean_absolute_error: 0.5525 - val_categorical_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5646 - mean_absolute_error: 0.5691 - categorical_accuracy: 1.0000 - val_loss: 0.5259 - val_mean_absolute_error: 0.5497 - val_categorical_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5580 - mean_absolute_error: 0.5682 - categorical_accuracy: 1.0000 - val_loss: 0.5142 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5509 - mean_absolute_error: 0.5654 - categorical_accuracy: 1.0000 - val_loss: 0.5023 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5397 - mean_absolute_error: 0.5607 - categorical_accuracy: 1.0000 - val_loss: 0.5013 - val_mean_absolute_error: 0.5413 - val_categorical_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5330 - mean_absolute_error: 0.5582 - categorical_accuracy: 1.0000 - val_loss: 0.4938 - val_mean_absolute_error: 0.5423 - val_categorical_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5257 - mean_absolute_error: 0.5569 - categorical_accuracy: 1.0000 - val_loss: 0.4888 - val_mean_absolute_error: 0.5418 - val_categorical_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5243 - mean_absolute_error: 0.5538 - categorical_accuracy: 1.0000 - val_loss: 0.4879 - val_mean_absolute_error: 0.5402 - val_categorical_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5199 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4846 - val_mean_absolute_error: 0.5404 - val_categorical_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5170 - mean_absolute_error: 0.5537 - categorical_accuracy: 1.0000 - val_loss: 0.4811 - val_mean_absolute_error: 0.5432 - val_categorical_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5144 - mean_absolute_error: 0.5531 - categorical_accuracy: 1.0000 - val_loss: 0.4818 - val_mean_absolute_error: 0.5436 - val_categorical_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5138 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4802 - val_mean_absolute_error: 0.5430 - val_categorical_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5199 - mean_absolute_error: 0.5589 - categorical_accuracy: 1.0000 - val_loss: 0.4776 - val_mean_absolute_error: 0.5444 - val_categorical_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5080 - mean_absolute_error: 0.5537 - categorical_accuracy: 1.0000 - val_loss: 0.4792 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5165 - mean_absolute_error: 0.5588 - categorical_accuracy: 1.0000 - val_loss: 0.4769 - val_mean_absolute_error: 0.5449 - val_categorical_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5170 - mean_absolute_error: 0.5576 - categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_mean_absolute_error: 0.5473 - val_categorical_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5095 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4754 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5159 - mean_absolute_error: 0.5612 - categorical_accuracy: 1.0000 - val_loss: 0.4740 - val_mean_absolute_error: 0.5444 - val_categorical_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5128 - mean_absolute_error: 0.5592 - categorical_accuracy: 1.0000 - val_loss: 0.4757 - val_mean_absolute_error: 0.5453 - val_categorical_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5193 - mean_absolute_error: 0.5608 - categorical_accuracy: 1.0000 - val_loss: 0.4764 - val_mean_absolute_error: 0.5473 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5118 - mean_absolute_error: 0.5576 - categorical_accuracy: 1.0000 - val_loss: 0.4747 - val_mean_absolute_error: 0.5484 - val_categorical_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5095 - mean_absolute_error: 0.5568 - categorical_accuracy: 1.0000 - val_loss: 0.4726 - val_mean_absolute_error: 0.5440 - val_categorical_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5104 - mean_absolute_error: 0.5558 - categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5090 - mean_absolute_error: 0.5574 - categorical_accuracy: 1.0000 - val_loss: 0.4773 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 0.5119 - mean_absolute_error: 0.5575 - categorical_accuracy: 1.0000 - val_loss: 0.4743 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5078 - mean_absolute_error: 0.5583 - categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_mean_absolute_error: 0.5462 - val_categorical_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5102 - mean_absolute_error: 0.5573 - categorical_accuracy: 1.0000 - val_loss: 0.4714 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5085 - mean_absolute_error: 0.5552 - categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_mean_absolute_error: 0.5451 - val_categorical_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5066 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4720 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5141 - mean_absolute_error: 0.5587 - categorical_accuracy: 1.0000 - val_loss: 0.4744 - val_mean_absolute_error: 0.5482 - val_categorical_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5087 - mean_absolute_error: 0.5572 - categorical_accuracy: 1.0000 - val_loss: 0.4746 - val_mean_absolute_error: 0.5470 - val_categorical_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5106 - mean_absolute_error: 0.5572 - categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_mean_absolute_error: 0.5445 - val_categorical_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5129 - mean_absolute_error: 0.5596 - categorical_accuracy: 1.0000 - val_loss: 0.4746 - val_mean_absolute_error: 0.5488 - val_categorical_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5076 - mean_absolute_error: 0.5569 - categorical_accuracy: 1.0000 - val_loss: 0.4727 - val_mean_absolute_error: 0.5459 - val_categorical_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5052 - mean_absolute_error: 0.5549 - categorical_accuracy: 1.0000 - val_loss: 0.4741 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5067 - mean_absolute_error: 0.5554 - categorical_accuracy: 1.0000 - val_loss: 0.4728 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5111 - mean_absolute_error: 0.5612 - categorical_accuracy: 1.0000 - val_loss: 0.4738 - val_mean_absolute_error: 0.5441 - val_categorical_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5071 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4739 - val_mean_absolute_error: 0.5453 - val_categorical_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5028 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5070 - mean_absolute_error: 0.5562 - categorical_accuracy: 1.0000 - val_loss: 0.4745 - val_mean_absolute_error: 0.5461 - val_categorical_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5050 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4721 - val_mean_absolute_error: 0.5439 - val_categorical_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5057 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4735 - val_mean_absolute_error: 0.5442 - val_categorical_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5117 - mean_absolute_error: 0.5584 - categorical_accuracy: 1.0000 - val_loss: 0.4755 - val_mean_absolute_error: 0.5437 - val_categorical_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5058 - mean_absolute_error: 0.5544 - categorical_accuracy: 1.0000 - val_loss: 0.4743 - val_mean_absolute_error: 0.5425 - val_categorical_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5088 - mean_absolute_error: 0.5562 - categorical_accuracy: 1.0000 - val_loss: 0.4739 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5051 - mean_absolute_error: 0.5575 - categorical_accuracy: 1.0000 - val_loss: 0.4770 - val_mean_absolute_error: 0.5439 - val_categorical_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5086 - mean_absolute_error: 0.5578 - categorical_accuracy: 1.0000 - val_loss: 0.4734 - val_mean_absolute_error: 0.5456 - val_categorical_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5054 - mean_absolute_error: 0.5528 - categorical_accuracy: 1.0000 - val_loss: 0.4728 - val_mean_absolute_error: 0.5467 - val_categorical_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5035 - mean_absolute_error: 0.5532 - categorical_accuracy: 1.0000 - val_loss: 0.4734 - val_mean_absolute_error: 0.5454 - val_categorical_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5051 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4752 - val_mean_absolute_error: 0.5476 - val_categorical_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5091 - mean_absolute_error: 0.5599 - categorical_accuracy: 1.0000 - val_loss: 0.4711 - val_mean_absolute_error: 0.5429 - val_categorical_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5066 - mean_absolute_error: 0.5570 - categorical_accuracy: 1.0000 - val_loss: 0.4709 - val_mean_absolute_error: 0.5455 - val_categorical_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5038 - mean_absolute_error: 0.5568 - categorical_accuracy: 1.0000 - val_loss: 0.4728 - val_mean_absolute_error: 0.5424 - val_categorical_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5021 - mean_absolute_error: 0.5557 - categorical_accuracy: 1.0000 - val_loss: 0.4711 - val_mean_absolute_error: 0.5449 - val_categorical_accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5036 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4685 - val_mean_absolute_error: 0.5424 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5064 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4656 - val_mean_absolute_error: 0.5436 - val_categorical_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5847/5847 [==============================] - 0s 60us/step - loss: 0.5026 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4709 - val_mean_absolute_error: 0.5438 - val_categorical_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5052 - mean_absolute_error: 0.5573 - categorical_accuracy: 1.0000 - val_loss: 0.4712 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5064 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4701 - val_mean_absolute_error: 0.5448 - val_categorical_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 0.5098 - mean_absolute_error: 0.5587 - categorical_accuracy: 1.0000 - val_loss: 0.4691 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 0.5043 - mean_absolute_error: 0.5561 - categorical_accuracy: 1.0000 - val_loss: 0.4695 - val_mean_absolute_error: 0.5429 - val_categorical_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5116 - mean_absolute_error: 0.5600 - categorical_accuracy: 1.0000 - val_loss: 0.4684 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5033 - mean_absolute_error: 0.5562 - categorical_accuracy: 1.0000 - val_loss: 0.4669 - val_mean_absolute_error: 0.5470 - val_categorical_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5063 - mean_absolute_error: 0.5588 - categorical_accuracy: 1.0000 - val_loss: 0.4714 - val_mean_absolute_error: 0.5471 - val_categorical_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5043 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4693 - val_mean_absolute_error: 0.5462 - val_categorical_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5038 - mean_absolute_error: 0.5557 - categorical_accuracy: 1.0000 - val_loss: 0.4698 - val_mean_absolute_error: 0.5484 - val_categorical_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.4979 - mean_absolute_error: 0.5549 - categorical_accuracy: 1.0000 - val_loss: 0.4725 - val_mean_absolute_error: 0.5451 - val_categorical_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5054 - mean_absolute_error: 0.5564 - categorical_accuracy: 1.0000 - val_loss: 0.4738 - val_mean_absolute_error: 0.5469 - val_categorical_accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5012 - mean_absolute_error: 0.5551 - categorical_accuracy: 1.0000 - val_loss: 0.4751 - val_mean_absolute_error: 0.5472 - val_categorical_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5046 - mean_absolute_error: 0.5558 - categorical_accuracy: 1.0000 - val_loss: 0.4709 - val_mean_absolute_error: 0.5474 - val_categorical_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5003 - mean_absolute_error: 0.5566 - categorical_accuracy: 1.0000 - val_loss: 0.4695 - val_mean_absolute_error: 0.5474 - val_categorical_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5004 - mean_absolute_error: 0.5544 - categorical_accuracy: 1.0000 - val_loss: 0.4699 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5030 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4694 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5030 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4700 - val_mean_absolute_error: 0.5469 - val_categorical_accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5007 - mean_absolute_error: 0.5549 - categorical_accuracy: 1.0000 - val_loss: 0.4702 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5009 - mean_absolute_error: 0.5548 - categorical_accuracy: 1.0000 - val_loss: 0.4694 - val_mean_absolute_error: 0.5416 - val_categorical_accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5039 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4676 - val_mean_absolute_error: 0.5446 - val_categorical_accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.4978 - mean_absolute_error: 0.5522 - categorical_accuracy: 1.0000 - val_loss: 0.4676 - val_mean_absolute_error: 0.5449 - val_categorical_accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5010 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4655 - val_mean_absolute_error: 0.5414 - val_categorical_accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5030 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4691 - val_mean_absolute_error: 0.5426 - val_categorical_accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5036 - mean_absolute_error: 0.5577 - categorical_accuracy: 1.0000 - val_loss: 0.4659 - val_mean_absolute_error: 0.5427 - val_categorical_accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5028 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4698 - val_mean_absolute_error: 0.5450 - val_categorical_accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5033 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4683 - val_mean_absolute_error: 0.5448 - val_categorical_accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5018 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4705 - val_mean_absolute_error: 0.5440 - val_categorical_accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5049 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4642 - val_mean_absolute_error: 0.5414 - val_categorical_accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5004 - mean_absolute_error: 0.5536 - categorical_accuracy: 1.0000 - val_loss: 0.4664 - val_mean_absolute_error: 0.5412 - val_categorical_accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.4967 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4658 - val_mean_absolute_error: 0.5430 - val_categorical_accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.5007 - mean_absolute_error: 0.5542 - categorical_accuracy: 1.0000 - val_loss: 0.4686 - val_mean_absolute_error: 0.5437 - val_categorical_accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.4990 - mean_absolute_error: 0.5536 - categorical_accuracy: 1.0000 - val_loss: 0.4680 - val_mean_absolute_error: 0.5447 - val_categorical_accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5031 - mean_absolute_error: 0.5564 - categorical_accuracy: 1.0000 - val_loss: 0.4704 - val_mean_absolute_error: 0.5456 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5047 - mean_absolute_error: 0.5565 - categorical_accuracy: 1.0000 - val_loss: 0.4660 - val_mean_absolute_error: 0.5446 - val_categorical_accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5015 - mean_absolute_error: 0.5583 - categorical_accuracy: 1.0000 - val_loss: 0.4659 - val_mean_absolute_error: 0.5433 - val_categorical_accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "5847/5847 [==============================] - 1s 94us/step - loss: 0.5022 - mean_absolute_error: 0.5591 - categorical_accuracy: 1.0000 - val_loss: 0.4694 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5050 - mean_absolute_error: 0.5593 - categorical_accuracy: 1.0000 - val_loss: 0.4664 - val_mean_absolute_error: 0.5448 - val_categorical_accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5033 - mean_absolute_error: 0.5567 - categorical_accuracy: 1.0000 - val_loss: 0.4684 - val_mean_absolute_error: 0.5447 - val_categorical_accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.4988 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4700 - val_mean_absolute_error: 0.5450 - val_categorical_accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5009 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4642 - val_mean_absolute_error: 0.5459 - val_categorical_accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5023 - mean_absolute_error: 0.5544 - categorical_accuracy: 1.0000 - val_loss: 0.4721 - val_mean_absolute_error: 0.5464 - val_categorical_accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 0.4998 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4681 - val_mean_absolute_error: 0.5460 - val_categorical_accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5000 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4656 - val_mean_absolute_error: 0.5419 - val_categorical_accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.4913 - mean_absolute_error: 0.5503 - categorical_accuracy: 1.0000 - val_loss: 0.4676 - val_mean_absolute_error: 0.5429 - val_categorical_accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5026 - mean_absolute_error: 0.5559 - categorical_accuracy: 1.0000 - val_loss: 0.4691 - val_mean_absolute_error: 0.5439 - val_categorical_accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 0.5069 - mean_absolute_error: 0.5590 - categorical_accuracy: 1.0000 - val_loss: 0.4659 - val_mean_absolute_error: 0.5412 - val_categorical_accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.4936 - mean_absolute_error: 0.5517 - categorical_accuracy: 1.0000 - val_loss: 0.4679 - val_mean_absolute_error: 0.5438 - val_categorical_accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 0.5012 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4646 - val_mean_absolute_error: 0.5428 - val_categorical_accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.4976 - mean_absolute_error: 0.5521 - categorical_accuracy: 1.0000 - val_loss: 0.4662 - val_mean_absolute_error: 0.5420 - val_categorical_accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 0.4976 - mean_absolute_error: 0.5527 - categorical_accuracy: 1.0000 - val_loss: 0.4713 - val_mean_absolute_error: 0.5451 - val_categorical_accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.4996 - mean_absolute_error: 0.5547 - categorical_accuracy: 1.0000 - val_loss: 0.4672 - val_mean_absolute_error: 0.5446 - val_categorical_accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 0.4997 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4692 - val_mean_absolute_error: 0.5454 - val_categorical_accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.5028 - mean_absolute_error: 0.5580 - categorical_accuracy: 1.0000 - val_loss: 0.4664 - val_mean_absolute_error: 0.5454 - val_categorical_accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 0.5011 - mean_absolute_error: 0.5568 - categorical_accuracy: 1.0000 - val_loss: 0.4681 - val_mean_absolute_error: 0.5445 - val_categorical_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5847/5847 [==============================] - 0s 75us/step - loss: 0.5066 - mean_absolute_error: 0.5579 - categorical_accuracy: 1.0000 - val_loss: 0.4672 - val_mean_absolute_error: 0.5432 - val_categorical_accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 0.4950 - mean_absolute_error: 0.5514 - categorical_accuracy: 1.0000 - val_loss: 0.4658 - val_mean_absolute_error: 0.5415 - val_categorical_accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5847/5847 [==============================] - 0s 81us/step - loss: 0.4982 - mean_absolute_error: 0.5536 - categorical_accuracy: 1.0000 - val_loss: 0.4637 - val_mean_absolute_error: 0.5410 - val_categorical_accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 0.4958 - mean_absolute_error: 0.5513 - categorical_accuracy: 1.0000 - val_loss: 0.4661 - val_mean_absolute_error: 0.5428 - val_categorical_accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.4976 - mean_absolute_error: 0.5535 - categorical_accuracy: 1.0000 - val_loss: 0.4638 - val_mean_absolute_error: 0.5404 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c11176eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winemod1.fit(x = X_train_scaled, y = y_train, epochs = 200,verbose=1, batch_size = 64,validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 0s 52us/step\n",
      "\n",
      "Loss = 0.46378947771512546\n",
      "Test Accuracy = 0.5404156184196472\n"
     ]
    }
   ],
   "source": [
    "preds = winemod1.evaluate(x = X_test_scaled, y = y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = winemod1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-369d25729a2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2558\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2560\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2561\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2562\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
