{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Read in white wine data \n",
    "white = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=';')\n",
    "\n",
    "# Read in red wine data \n",
    "red = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add `type` column to `red` with value 1\n",
    "red['type'] = 1\n",
    "\n",
    "# Add `type` column to `white` with value 0\n",
    "white['type'] = 0\n",
    "\n",
    "# Append `white` to `red`\n",
    "wines = red.append(white, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wines.quality\n",
    "X = wines.drop(['quality', 'residual sugar', 'free sulfur dioxide', 'type'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=46, stratify=y)\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Scale the train set\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=200, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpregressor = MLPRegressor(hidden_layer_sizes=(200), max_iter=500, alpha = 0.001)  \n",
    "mlpregressor.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>6</td>\n",
       "      <td>5.911639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>6</td>\n",
       "      <td>6.097940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5040</th>\n",
       "      <td>5</td>\n",
       "      <td>6.215848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>7</td>\n",
       "      <td>6.427831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>7</td>\n",
       "      <td>6.129530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>6</td>\n",
       "      <td>6.353232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>5</td>\n",
       "      <td>5.194208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>6</td>\n",
       "      <td>5.391799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>5</td>\n",
       "      <td>5.282166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6</td>\n",
       "      <td>5.624213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>7</td>\n",
       "      <td>6.861119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>7</td>\n",
       "      <td>5.814858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>5</td>\n",
       "      <td>5.915157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>6</td>\n",
       "      <td>5.809701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>5</td>\n",
       "      <td>5.356301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6402</th>\n",
       "      <td>7</td>\n",
       "      <td>6.238553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>6</td>\n",
       "      <td>6.215741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>6</td>\n",
       "      <td>6.304207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>6</td>\n",
       "      <td>6.249108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>7</td>\n",
       "      <td>6.698387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4</td>\n",
       "      <td>5.567998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>8</td>\n",
       "      <td>6.167997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>6</td>\n",
       "      <td>6.414853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>5</td>\n",
       "      <td>5.320675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5</td>\n",
       "      <td>5.198935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>7</td>\n",
       "      <td>7.107898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>6</td>\n",
       "      <td>6.789814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>6</td>\n",
       "      <td>5.975168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6123</th>\n",
       "      <td>6</td>\n",
       "      <td>5.879433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>7</td>\n",
       "      <td>6.476813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>6</td>\n",
       "      <td>5.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>5</td>\n",
       "      <td>5.630922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>5</td>\n",
       "      <td>5.320336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>6</td>\n",
       "      <td>6.816310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>6</td>\n",
       "      <td>5.434639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>6</td>\n",
       "      <td>6.236113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5</td>\n",
       "      <td>5.152889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>7</td>\n",
       "      <td>6.329580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6197</th>\n",
       "      <td>6</td>\n",
       "      <td>6.030957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>7</td>\n",
       "      <td>5.891319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>6</td>\n",
       "      <td>5.842822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>6</td>\n",
       "      <td>5.886916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>5</td>\n",
       "      <td>5.511749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>6</td>\n",
       "      <td>6.413320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>5</td>\n",
       "      <td>5.291676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>5</td>\n",
       "      <td>4.529835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>6</td>\n",
       "      <td>5.713771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>7</td>\n",
       "      <td>6.077339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6</td>\n",
       "      <td>5.259983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>6</td>\n",
       "      <td>5.465405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>5</td>\n",
       "      <td>5.054113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>6</td>\n",
       "      <td>5.453788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>6</td>\n",
       "      <td>6.348817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>5</td>\n",
       "      <td>5.233881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>6</td>\n",
       "      <td>6.051540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>6</td>\n",
       "      <td>5.904457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>4</td>\n",
       "      <td>5.346079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>5</td>\n",
       "      <td>5.201867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>6</td>\n",
       "      <td>6.063610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>5</td>\n",
       "      <td>6.010210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted\n",
       "1639       6   5.911639\n",
       "5140       6   6.097940\n",
       "5040       5   6.215848\n",
       "3383       7   6.427831\n",
       "5176       7   6.129530\n",
       "4022       6   6.353232\n",
       "2806       5   5.194208\n",
       "1779       6   5.391799\n",
       "1262       5   5.282166\n",
       "116        6   5.624213\n",
       "1845       7   6.861119\n",
       "2519       7   5.814858\n",
       "1247       5   5.915157\n",
       "5800       6   5.809701\n",
       "417        5   5.356301\n",
       "6402       7   6.238553\n",
       "1490       6   6.215741\n",
       "4813       6   6.304207\n",
       "5149       6   6.249108\n",
       "4921       7   6.698387\n",
       "4092       4   5.567998\n",
       "5088       8   6.167997\n",
       "1126       6   6.414853\n",
       "848        5   5.320675\n",
       "2001       5   5.198935\n",
       "5875       7   7.107898\n",
       "2003       6   6.789814\n",
       "1390       6   5.975168\n",
       "6123       6   5.879433\n",
       "4256       7   6.476813\n",
       "...      ...        ...\n",
       "2293       6   5.533838\n",
       "5955       5   5.630922\n",
       "1241       5   5.320336\n",
       "1172       6   6.816310\n",
       "2380       6   5.434639\n",
       "5305       6   6.236113\n",
       "183        5   5.152889\n",
       "514        7   6.329580\n",
       "6197       6   6.030957\n",
       "5217       7   5.891319\n",
       "4571       6   5.842822\n",
       "4056       6   5.886916\n",
       "611        5   5.511749\n",
       "5650       6   6.413320\n",
       "3878       5   5.291676\n",
       "4253       5   4.529835\n",
       "382        6   5.713771\n",
       "2557       7   6.077339\n",
       "91         6   5.259983\n",
       "566        6   5.465405\n",
       "761        5   5.054113\n",
       "426        6   5.453788\n",
       "1580       6   6.348817\n",
       "105        5   5.233881\n",
       "5533       6   6.051540\n",
       "2347       6   5.904457\n",
       "2301       4   5.346079\n",
       "993        5   5.201867\n",
       "466        6   6.063610\n",
       "3064       5   6.010210\n",
       "\n",
       "[650 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlpregressor.predict(X_test_scaled)\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5387717828829957\n",
      "Mean Squared Error: 0.48090349957433126\n",
      "Root Mean Squared Error: 0.6934720611346439\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpregressor.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn import preprocessing, model_selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "winemod1 = Sequential()\n",
    "# layer 1\n",
    "winemod1.add(Dense(9, input_dim=9, activation='relu', name='fc0',kernel_regularizer=regularizers.l2(0.001)))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "#layer 2\n",
    "winemod1.add(Dense(50, name='fc1',bias_initializer='zeros'))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "winemod1.add(Activation('relu'))\n",
    "winemod1.add(Dropout(0.5))\n",
    "#layer 3\n",
    "winemod1.add(Dense(100, name='fc2',bias_initializer='zeros'))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "winemod1.add(Activation('relu'))\n",
    "winemod1.add(Dropout(0.5))\n",
    "#layer 4\n",
    "winemod1.add(Dense(1, name='fc3',bias_initializer='zeros'))\n",
    "winemod1.add(BatchNormalization(momentum=0.99, epsilon=0.001))\n",
    "winemod1.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc0 (Dense)                  (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9)                 36        \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 50)                500       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 6,431\n",
      "Trainable params: 6,111\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "winemod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "Adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "winemod1.compile(optimizer = Adam, loss='mean_squared_error', metrics=[metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5847 samples, validate on 650 samples\n",
      "Epoch 1/200\n",
      "5847/5847 [==============================] - 1s 216us/step - loss: 34.4980 - mean_absolute_error: 5.7738 - categorical_accuracy: 1.0000 - val_loss: 32.1556 - val_mean_absolute_error: 5.5872 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 33.2088 - mean_absolute_error: 5.6825 - categorical_accuracy: 1.0000 - val_loss: 31.5803 - val_mean_absolute_error: 5.5496 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 32.0767 - mean_absolute_error: 5.5917 - categorical_accuracy: 1.0000 - val_loss: 30.9824 - val_mean_absolute_error: 5.5070 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 30.9963 - mean_absolute_error: 5.5016 - categorical_accuracy: 1.0000 - val_loss: 29.9570 - val_mean_absolute_error: 5.4188 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 29.9682 - mean_absolute_error: 5.4122 - categorical_accuracy: 1.0000 - val_loss: 28.9160 - val_mean_absolute_error: 5.3252 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 28.9861 - mean_absolute_error: 5.3235 - categorical_accuracy: 1.0000 - val_loss: 27.9244 - val_mean_absolute_error: 5.2328 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 28.0265 - mean_absolute_error: 5.2355 - categorical_accuracy: 1.0000 - val_loss: 27.0040 - val_mean_absolute_error: 5.1452 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 27.1057 - mean_absolute_error: 5.1481 - categorical_accuracy: 1.0000 - val_loss: 26.3915 - val_mean_absolute_error: 5.0859 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 26.2051 - mean_absolute_error: 5.0613 - categorical_accuracy: 1.0000 - val_loss: 25.3924 - val_mean_absolute_error: 4.9871 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 25.3424 - mean_absolute_error: 4.9751 - categorical_accuracy: 1.0000 - val_loss: 24.6659 - val_mean_absolute_error: 4.9139 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 24.4859 - mean_absolute_error: 4.8895 - categorical_accuracy: 1.0000 - val_loss: 23.7848 - val_mean_absolute_error: 4.8234 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "5847/5847 [==============================] - ETA: 0s - loss: 23.6515 - mean_absolute_error: 4.8034 - categorical_accuracy: 1.000 - 0s 79us/step - loss: 23.6605 - mean_absolute_error: 4.8045 - categorical_accuracy: 1.0000 - val_loss: 22.9484 - val_mean_absolute_error: 4.7359 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 22.8479 - mean_absolute_error: 4.7198 - categorical_accuracy: 1.0000 - val_loss: 22.2132 - val_mean_absolute_error: 4.6578 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 22.0601 - mean_absolute_error: 4.6360 - categorical_accuracy: 1.0000 - val_loss: 21.3673 - val_mean_absolute_error: 4.5661 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 21.2961 - mean_absolute_error: 4.5528 - categorical_accuracy: 1.0000 - val_loss: 20.6316 - val_mean_absolute_error: 4.4849 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 20.5476 - mean_absolute_error: 4.4699 - categorical_accuracy: 1.0000 - val_loss: 19.9362 - val_mean_absolute_error: 4.4067 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 19.8141 - mean_absolute_error: 4.3876 - categorical_accuracy: 1.0000 - val_loss: 19.3364 - val_mean_absolute_error: 4.3384 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "5847/5847 [==============================] - 0s 83us/step - loss: 19.1008 - mean_absolute_error: 4.3059 - categorical_accuracy: 1.0000 - val_loss: 18.6459 - val_mean_absolute_error: 4.2581 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 18.4057 - mean_absolute_error: 4.2248 - categorical_accuracy: 1.0000 - val_loss: 17.9284 - val_mean_absolute_error: 4.1731 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 17.7278 - mean_absolute_error: 4.1440 - categorical_accuracy: 1.0000 - val_loss: 17.1884 - val_mean_absolute_error: 4.0837 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 17.0712 - mean_absolute_error: 4.0638 - categorical_accuracy: 1.0000 - val_loss: 16.5603 - val_mean_absolute_error: 4.0059 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "5847/5847 [==============================] - 0s 75us/step - loss: 16.4313 - mean_absolute_error: 3.9842 - categorical_accuracy: 1.0000 - val_loss: 15.9222 - val_mean_absolute_error: 3.9256 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 15.7995 - mean_absolute_error: 3.9049 - categorical_accuracy: 1.0000 - val_loss: 15.3341 - val_mean_absolute_error: 3.8500 - val_categorical_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 15.1931 - mean_absolute_error: 3.8262 - categorical_accuracy: 1.0000 - val_loss: 14.7605 - val_mean_absolute_error: 3.7749 - val_categorical_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "5847/5847 [==============================] - 0s 80us/step - loss: 14.6009 - mean_absolute_error: 3.7481 - categorical_accuracy: 1.0000 - val_loss: 14.2133 - val_mean_absolute_error: 3.7020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 14.0202 - mean_absolute_error: 3.6703 - categorical_accuracy: 1.0000 - val_loss: 13.6633 - val_mean_absolute_error: 3.6270 - val_categorical_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 13.4549 - mean_absolute_error: 3.5928 - categorical_accuracy: 1.0000 - val_loss: 13.1008 - val_mean_absolute_error: 3.5489 - val_categorical_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "5847/5847 [==============================] - 0s 75us/step - loss: 12.9098 - mean_absolute_error: 3.5161 - categorical_accuracy: 1.0000 - val_loss: 12.5090 - val_mean_absolute_error: 3.4648 - val_categorical_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 12.3761 - mean_absolute_error: 3.4399 - categorical_accuracy: 1.0000 - val_loss: 11.9523 - val_mean_absolute_error: 3.3835 - val_categorical_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 11.8599 - mean_absolute_error: 3.3641 - categorical_accuracy: 1.0000 - val_loss: 11.5273 - val_mean_absolute_error: 3.3206 - val_categorical_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 11.3609 - mean_absolute_error: 3.2885 - categorical_accuracy: 1.0000 - val_loss: 10.9287 - val_mean_absolute_error: 3.2293 - val_categorical_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 10.8687 - mean_absolute_error: 3.2141 - categorical_accuracy: 1.0000 - val_loss: 10.5609 - val_mean_absolute_error: 3.1719 - val_categorical_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 10.3997 - mean_absolute_error: 3.1397 - categorical_accuracy: 1.0000 - val_loss: 10.0635 - val_mean_absolute_error: 3.0925 - val_categorical_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 9.9406 - mean_absolute_error: 3.0658 - categorical_accuracy: 1.0000 - val_loss: 9.6110 - val_mean_absolute_error: 3.0183 - val_categorical_accuracy: 1.0000\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5847/5847 [==============================] - 0s 69us/step - loss: 9.4945 - mean_absolute_error: 2.9925 - categorical_accuracy: 1.0000 - val_loss: 9.1193 - val_mean_absolute_error: 2.9355 - val_categorical_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 9.0590 - mean_absolute_error: 2.9199 - categorical_accuracy: 1.0000 - val_loss: 8.7318 - val_mean_absolute_error: 2.8692 - val_categorical_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 8.6401 - mean_absolute_error: 2.8475 - categorical_accuracy: 1.0000 - val_loss: 8.3048 - val_mean_absolute_error: 2.7940 - val_categorical_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 8.2371 - mean_absolute_error: 2.7759 - categorical_accuracy: 1.0000 - val_loss: 7.8861 - val_mean_absolute_error: 2.7179 - val_categorical_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 7.8439 - mean_absolute_error: 2.7043 - categorical_accuracy: 1.0000 - val_loss: 7.5678 - val_mean_absolute_error: 2.6588 - val_categorical_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 7.4648 - mean_absolute_error: 2.6335 - categorical_accuracy: 1.0000 - val_loss: 7.2751 - val_mean_absolute_error: 2.6036 - val_categorical_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 7.0958 - mean_absolute_error: 2.5634 - categorical_accuracy: 1.0000 - val_loss: 6.9030 - val_mean_absolute_error: 2.5313 - val_categorical_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 6.7423 - mean_absolute_error: 2.4940 - categorical_accuracy: 1.0000 - val_loss: 6.5597 - val_mean_absolute_error: 2.4630 - val_categorical_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 6.4002 - mean_absolute_error: 2.4252 - categorical_accuracy: 1.0000 - val_loss: 6.1890 - val_mean_absolute_error: 2.3873 - val_categorical_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 6.0727 - mean_absolute_error: 2.3568 - categorical_accuracy: 1.0000 - val_loss: 5.8656 - val_mean_absolute_error: 2.3193 - val_categorical_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 5.7557 - mean_absolute_error: 2.2891 - categorical_accuracy: 1.0000 - val_loss: 5.5075 - val_mean_absolute_error: 2.2411 - val_categorical_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 5.4506 - mean_absolute_error: 2.2217 - categorical_accuracy: 1.0000 - val_loss: 5.2526 - val_mean_absolute_error: 2.1837 - val_categorical_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 5.1555 - mean_absolute_error: 2.1553 - categorical_accuracy: 1.0000 - val_loss: 4.9145 - val_mean_absolute_error: 2.1054 - val_categorical_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 4.8678 - mean_absolute_error: 2.0900 - categorical_accuracy: 1.0000 - val_loss: 4.7281 - val_mean_absolute_error: 2.0616 - val_categorical_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 4.6092 - mean_absolute_error: 2.0256 - categorical_accuracy: 1.0000 - val_loss: 4.4648 - val_mean_absolute_error: 1.9972 - val_categorical_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 4.3435 - mean_absolute_error: 1.9611 - categorical_accuracy: 1.0000 - val_loss: 4.2067 - val_mean_absolute_error: 1.9326 - val_categorical_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 4.0952 - mean_absolute_error: 1.8972 - categorical_accuracy: 1.0000 - val_loss: 3.9378 - val_mean_absolute_error: 1.8632 - val_categorical_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 3.8559 - mean_absolute_error: 1.8349 - categorical_accuracy: 1.0000 - val_loss: 3.7022 - val_mean_absolute_error: 1.7998 - val_categorical_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 3.6239 - mean_absolute_error: 1.7728 - categorical_accuracy: 1.0000 - val_loss: 3.4602 - val_mean_absolute_error: 1.7323 - val_categorical_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 3.4082 - mean_absolute_error: 1.7115 - categorical_accuracy: 1.0000 - val_loss: 3.2517 - val_mean_absolute_error: 1.6726 - val_categorical_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 3.2049 - mean_absolute_error: 1.6538 - categorical_accuracy: 1.0000 - val_loss: 3.0351 - val_mean_absolute_error: 1.6071 - val_categorical_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 3.0091 - mean_absolute_error: 1.5941 - categorical_accuracy: 1.0000 - val_loss: 2.8558 - val_mean_absolute_error: 1.5531 - val_categorical_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 2.8195 - mean_absolute_error: 1.5350 - categorical_accuracy: 1.0000 - val_loss: 2.6852 - val_mean_absolute_error: 1.4978 - val_categorical_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 2.6396 - mean_absolute_error: 1.4776 - categorical_accuracy: 1.0000 - val_loss: 2.5106 - val_mean_absolute_error: 1.4408 - val_categorical_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 2.4702 - mean_absolute_error: 1.4221 - categorical_accuracy: 1.0000 - val_loss: 2.3463 - val_mean_absolute_error: 1.3837 - val_categorical_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 2.3167 - mean_absolute_error: 1.3678 - categorical_accuracy: 1.0000 - val_loss: 2.2201 - val_mean_absolute_error: 1.3405 - val_categorical_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 2.1679 - mean_absolute_error: 1.3143 - categorical_accuracy: 1.0000 - val_loss: 2.0322 - val_mean_absolute_error: 1.2719 - val_categorical_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 2.0213 - mean_absolute_error: 1.2629 - categorical_accuracy: 1.0000 - val_loss: 1.9560 - val_mean_absolute_error: 1.2432 - val_categorical_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 1.8895 - mean_absolute_error: 1.2126 - categorical_accuracy: 1.0000 - val_loss: 1.7724 - val_mean_absolute_error: 1.1723 - val_categorical_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 1.7674 - mean_absolute_error: 1.1641 - categorical_accuracy: 1.0000 - val_loss: 1.6733 - val_mean_absolute_error: 1.1347 - val_categorical_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 1.6482 - mean_absolute_error: 1.1164 - categorical_accuracy: 1.0000 - val_loss: 1.5730 - val_mean_absolute_error: 1.0918 - val_categorical_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 1.5383 - mean_absolute_error: 1.0700 - categorical_accuracy: 1.0000 - val_loss: 1.4380 - val_mean_absolute_error: 1.0341 - val_categorical_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 1.4419 - mean_absolute_error: 1.0263 - categorical_accuracy: 1.0000 - val_loss: 1.3594 - val_mean_absolute_error: 0.9987 - val_categorical_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 1.3390 - mean_absolute_error: 0.9821 - categorical_accuracy: 1.0000 - val_loss: 1.2630 - val_mean_absolute_error: 0.9546 - val_categorical_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 1.2542 - mean_absolute_error: 0.9428 - categorical_accuracy: 1.0000 - val_loss: 1.1547 - val_mean_absolute_error: 0.9018 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 1.1817 - mean_absolute_error: 0.9067 - categorical_accuracy: 1.0000 - val_loss: 1.0783 - val_mean_absolute_error: 0.8627 - val_categorical_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 1.1056 - mean_absolute_error: 0.8698 - categorical_accuracy: 1.0000 - val_loss: 1.0229 - val_mean_absolute_error: 0.8339 - val_categorical_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 1.0329 - mean_absolute_error: 0.8336 - categorical_accuracy: 1.0000 - val_loss: 0.9707 - val_mean_absolute_error: 0.8052 - val_categorical_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 0.9695 - mean_absolute_error: 0.7993 - categorical_accuracy: 1.0000 - val_loss: 0.8948 - val_mean_absolute_error: 0.7626 - val_categorical_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.9159 - mean_absolute_error: 0.7683 - categorical_accuracy: 1.0000 - val_loss: 0.8534 - val_mean_absolute_error: 0.7386 - val_categorical_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.8634 - mean_absolute_error: 0.7396 - categorical_accuracy: 1.0000 - val_loss: 0.8059 - val_mean_absolute_error: 0.7098 - val_categorical_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.8146 - mean_absolute_error: 0.7131 - categorical_accuracy: 1.0000 - val_loss: 0.7560 - val_mean_absolute_error: 0.6810 - val_categorical_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.7756 - mean_absolute_error: 0.6900 - categorical_accuracy: 1.0000 - val_loss: 0.7293 - val_mean_absolute_error: 0.6639 - val_categorical_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.7360 - mean_absolute_error: 0.6676 - categorical_accuracy: 1.0000 - val_loss: 0.6817 - val_mean_absolute_error: 0.6356 - val_categorical_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.7049 - mean_absolute_error: 0.6473 - categorical_accuracy: 1.0000 - val_loss: 0.6452 - val_mean_absolute_error: 0.6132 - val_categorical_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 0.6794 - mean_absolute_error: 0.6317 - categorical_accuracy: 1.0000 - val_loss: 0.6374 - val_mean_absolute_error: 0.6090 - val_categorical_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.6537 - mean_absolute_error: 0.6170 - categorical_accuracy: 1.0000 - val_loss: 0.6037 - val_mean_absolute_error: 0.5870 - val_categorical_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.6297 - mean_absolute_error: 0.6051 - categorical_accuracy: 1.0000 - val_loss: 0.5845 - val_mean_absolute_error: 0.5769 - val_categorical_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.6049 - mean_absolute_error: 0.5905 - categorical_accuracy: 1.0000 - val_loss: 0.5538 - val_mean_absolute_error: 0.5596 - val_categorical_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5957 - mean_absolute_error: 0.5851 - categorical_accuracy: 1.0000 - val_loss: 0.5458 - val_mean_absolute_error: 0.5553 - val_categorical_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5741 - mean_absolute_error: 0.5747 - categorical_accuracy: 1.0000 - val_loss: 0.5323 - val_mean_absolute_error: 0.5525 - val_categorical_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5646 - mean_absolute_error: 0.5691 - categorical_accuracy: 1.0000 - val_loss: 0.5259 - val_mean_absolute_error: 0.5497 - val_categorical_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5580 - mean_absolute_error: 0.5682 - categorical_accuracy: 1.0000 - val_loss: 0.5142 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5509 - mean_absolute_error: 0.5654 - categorical_accuracy: 1.0000 - val_loss: 0.5023 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5397 - mean_absolute_error: 0.5607 - categorical_accuracy: 1.0000 - val_loss: 0.5013 - val_mean_absolute_error: 0.5413 - val_categorical_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5330 - mean_absolute_error: 0.5582 - categorical_accuracy: 1.0000 - val_loss: 0.4938 - val_mean_absolute_error: 0.5423 - val_categorical_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5257 - mean_absolute_error: 0.5569 - categorical_accuracy: 1.0000 - val_loss: 0.4888 - val_mean_absolute_error: 0.5418 - val_categorical_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5243 - mean_absolute_error: 0.5538 - categorical_accuracy: 1.0000 - val_loss: 0.4879 - val_mean_absolute_error: 0.5402 - val_categorical_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5199 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4846 - val_mean_absolute_error: 0.5404 - val_categorical_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5170 - mean_absolute_error: 0.5537 - categorical_accuracy: 1.0000 - val_loss: 0.4811 - val_mean_absolute_error: 0.5432 - val_categorical_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5144 - mean_absolute_error: 0.5531 - categorical_accuracy: 1.0000 - val_loss: 0.4818 - val_mean_absolute_error: 0.5436 - val_categorical_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5138 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4802 - val_mean_absolute_error: 0.5430 - val_categorical_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5199 - mean_absolute_error: 0.5589 - categorical_accuracy: 1.0000 - val_loss: 0.4776 - val_mean_absolute_error: 0.5444 - val_categorical_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5080 - mean_absolute_error: 0.5537 - categorical_accuracy: 1.0000 - val_loss: 0.4792 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5165 - mean_absolute_error: 0.5588 - categorical_accuracy: 1.0000 - val_loss: 0.4769 - val_mean_absolute_error: 0.5449 - val_categorical_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5170 - mean_absolute_error: 0.5576 - categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_mean_absolute_error: 0.5473 - val_categorical_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5095 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4754 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5159 - mean_absolute_error: 0.5612 - categorical_accuracy: 1.0000 - val_loss: 0.4740 - val_mean_absolute_error: 0.5444 - val_categorical_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5128 - mean_absolute_error: 0.5592 - categorical_accuracy: 1.0000 - val_loss: 0.4757 - val_mean_absolute_error: 0.5453 - val_categorical_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5193 - mean_absolute_error: 0.5608 - categorical_accuracy: 1.0000 - val_loss: 0.4764 - val_mean_absolute_error: 0.5473 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5118 - mean_absolute_error: 0.5576 - categorical_accuracy: 1.0000 - val_loss: 0.4747 - val_mean_absolute_error: 0.5484 - val_categorical_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5095 - mean_absolute_error: 0.5568 - categorical_accuracy: 1.0000 - val_loss: 0.4726 - val_mean_absolute_error: 0.5440 - val_categorical_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5104 - mean_absolute_error: 0.5558 - categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5090 - mean_absolute_error: 0.5574 - categorical_accuracy: 1.0000 - val_loss: 0.4773 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 0.5119 - mean_absolute_error: 0.5575 - categorical_accuracy: 1.0000 - val_loss: 0.4743 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5078 - mean_absolute_error: 0.5583 - categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_mean_absolute_error: 0.5462 - val_categorical_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5102 - mean_absolute_error: 0.5573 - categorical_accuracy: 1.0000 - val_loss: 0.4714 - val_mean_absolute_error: 0.5463 - val_categorical_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5085 - mean_absolute_error: 0.5552 - categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_mean_absolute_error: 0.5451 - val_categorical_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5066 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4720 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5141 - mean_absolute_error: 0.5587 - categorical_accuracy: 1.0000 - val_loss: 0.4744 - val_mean_absolute_error: 0.5482 - val_categorical_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5087 - mean_absolute_error: 0.5572 - categorical_accuracy: 1.0000 - val_loss: 0.4746 - val_mean_absolute_error: 0.5470 - val_categorical_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5106 - mean_absolute_error: 0.5572 - categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_mean_absolute_error: 0.5445 - val_categorical_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5129 - mean_absolute_error: 0.5596 - categorical_accuracy: 1.0000 - val_loss: 0.4746 - val_mean_absolute_error: 0.5488 - val_categorical_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5076 - mean_absolute_error: 0.5569 - categorical_accuracy: 1.0000 - val_loss: 0.4727 - val_mean_absolute_error: 0.5459 - val_categorical_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5052 - mean_absolute_error: 0.5549 - categorical_accuracy: 1.0000 - val_loss: 0.4741 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5067 - mean_absolute_error: 0.5554 - categorical_accuracy: 1.0000 - val_loss: 0.4728 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5111 - mean_absolute_error: 0.5612 - categorical_accuracy: 1.0000 - val_loss: 0.4738 - val_mean_absolute_error: 0.5441 - val_categorical_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5071 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4739 - val_mean_absolute_error: 0.5453 - val_categorical_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5028 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5070 - mean_absolute_error: 0.5562 - categorical_accuracy: 1.0000 - val_loss: 0.4745 - val_mean_absolute_error: 0.5461 - val_categorical_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5050 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4721 - val_mean_absolute_error: 0.5439 - val_categorical_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5057 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4735 - val_mean_absolute_error: 0.5442 - val_categorical_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5117 - mean_absolute_error: 0.5584 - categorical_accuracy: 1.0000 - val_loss: 0.4755 - val_mean_absolute_error: 0.5437 - val_categorical_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5058 - mean_absolute_error: 0.5544 - categorical_accuracy: 1.0000 - val_loss: 0.4743 - val_mean_absolute_error: 0.5425 - val_categorical_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5088 - mean_absolute_error: 0.5562 - categorical_accuracy: 1.0000 - val_loss: 0.4739 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5051 - mean_absolute_error: 0.5575 - categorical_accuracy: 1.0000 - val_loss: 0.4770 - val_mean_absolute_error: 0.5439 - val_categorical_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5086 - mean_absolute_error: 0.5578 - categorical_accuracy: 1.0000 - val_loss: 0.4734 - val_mean_absolute_error: 0.5456 - val_categorical_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5054 - mean_absolute_error: 0.5528 - categorical_accuracy: 1.0000 - val_loss: 0.4728 - val_mean_absolute_error: 0.5467 - val_categorical_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5035 - mean_absolute_error: 0.5532 - categorical_accuracy: 1.0000 - val_loss: 0.4734 - val_mean_absolute_error: 0.5454 - val_categorical_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5051 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4752 - val_mean_absolute_error: 0.5476 - val_categorical_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5091 - mean_absolute_error: 0.5599 - categorical_accuracy: 1.0000 - val_loss: 0.4711 - val_mean_absolute_error: 0.5429 - val_categorical_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5066 - mean_absolute_error: 0.5570 - categorical_accuracy: 1.0000 - val_loss: 0.4709 - val_mean_absolute_error: 0.5455 - val_categorical_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5038 - mean_absolute_error: 0.5568 - categorical_accuracy: 1.0000 - val_loss: 0.4728 - val_mean_absolute_error: 0.5424 - val_categorical_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5021 - mean_absolute_error: 0.5557 - categorical_accuracy: 1.0000 - val_loss: 0.4711 - val_mean_absolute_error: 0.5449 - val_categorical_accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5036 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4685 - val_mean_absolute_error: 0.5424 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5064 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4656 - val_mean_absolute_error: 0.5436 - val_categorical_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5847/5847 [==============================] - 0s 60us/step - loss: 0.5026 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4709 - val_mean_absolute_error: 0.5438 - val_categorical_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5052 - mean_absolute_error: 0.5573 - categorical_accuracy: 1.0000 - val_loss: 0.4712 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5064 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4701 - val_mean_absolute_error: 0.5448 - val_categorical_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "5847/5847 [==============================] - 0s 62us/step - loss: 0.5098 - mean_absolute_error: 0.5587 - categorical_accuracy: 1.0000 - val_loss: 0.4691 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "5847/5847 [==============================] - 0s 61us/step - loss: 0.5043 - mean_absolute_error: 0.5561 - categorical_accuracy: 1.0000 - val_loss: 0.4695 - val_mean_absolute_error: 0.5429 - val_categorical_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5116 - mean_absolute_error: 0.5600 - categorical_accuracy: 1.0000 - val_loss: 0.4684 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5033 - mean_absolute_error: 0.5562 - categorical_accuracy: 1.0000 - val_loss: 0.4669 - val_mean_absolute_error: 0.5470 - val_categorical_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5063 - mean_absolute_error: 0.5588 - categorical_accuracy: 1.0000 - val_loss: 0.4714 - val_mean_absolute_error: 0.5471 - val_categorical_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5043 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4693 - val_mean_absolute_error: 0.5462 - val_categorical_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.5038 - mean_absolute_error: 0.5557 - categorical_accuracy: 1.0000 - val_loss: 0.4698 - val_mean_absolute_error: 0.5484 - val_categorical_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.4979 - mean_absolute_error: 0.5549 - categorical_accuracy: 1.0000 - val_loss: 0.4725 - val_mean_absolute_error: 0.5451 - val_categorical_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5054 - mean_absolute_error: 0.5564 - categorical_accuracy: 1.0000 - val_loss: 0.4738 - val_mean_absolute_error: 0.5469 - val_categorical_accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5012 - mean_absolute_error: 0.5551 - categorical_accuracy: 1.0000 - val_loss: 0.4751 - val_mean_absolute_error: 0.5472 - val_categorical_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5046 - mean_absolute_error: 0.5558 - categorical_accuracy: 1.0000 - val_loss: 0.4709 - val_mean_absolute_error: 0.5474 - val_categorical_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5003 - mean_absolute_error: 0.5566 - categorical_accuracy: 1.0000 - val_loss: 0.4695 - val_mean_absolute_error: 0.5474 - val_categorical_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5004 - mean_absolute_error: 0.5544 - categorical_accuracy: 1.0000 - val_loss: 0.4699 - val_mean_absolute_error: 0.5468 - val_categorical_accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5030 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4694 - val_mean_absolute_error: 0.5458 - val_categorical_accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5847/5847 [==============================] - 0s 64us/step - loss: 0.5030 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4700 - val_mean_absolute_error: 0.5469 - val_categorical_accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5007 - mean_absolute_error: 0.5549 - categorical_accuracy: 1.0000 - val_loss: 0.4702 - val_mean_absolute_error: 0.5457 - val_categorical_accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5009 - mean_absolute_error: 0.5548 - categorical_accuracy: 1.0000 - val_loss: 0.4694 - val_mean_absolute_error: 0.5416 - val_categorical_accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5039 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4676 - val_mean_absolute_error: 0.5446 - val_categorical_accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.4978 - mean_absolute_error: 0.5522 - categorical_accuracy: 1.0000 - val_loss: 0.4676 - val_mean_absolute_error: 0.5449 - val_categorical_accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5010 - mean_absolute_error: 0.5541 - categorical_accuracy: 1.0000 - val_loss: 0.4655 - val_mean_absolute_error: 0.5414 - val_categorical_accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5030 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4691 - val_mean_absolute_error: 0.5426 - val_categorical_accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5036 - mean_absolute_error: 0.5577 - categorical_accuracy: 1.0000 - val_loss: 0.4659 - val_mean_absolute_error: 0.5427 - val_categorical_accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5847/5847 [==============================] - 0s 68us/step - loss: 0.5028 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4698 - val_mean_absolute_error: 0.5450 - val_categorical_accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "5847/5847 [==============================] - 0s 67us/step - loss: 0.5033 - mean_absolute_error: 0.5555 - categorical_accuracy: 1.0000 - val_loss: 0.4683 - val_mean_absolute_error: 0.5448 - val_categorical_accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5018 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4705 - val_mean_absolute_error: 0.5440 - val_categorical_accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5049 - mean_absolute_error: 0.5553 - categorical_accuracy: 1.0000 - val_loss: 0.4642 - val_mean_absolute_error: 0.5414 - val_categorical_accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5847/5847 [==============================] - 0s 70us/step - loss: 0.5004 - mean_absolute_error: 0.5536 - categorical_accuracy: 1.0000 - val_loss: 0.4664 - val_mean_absolute_error: 0.5412 - val_categorical_accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.4967 - mean_absolute_error: 0.5540 - categorical_accuracy: 1.0000 - val_loss: 0.4658 - val_mean_absolute_error: 0.5430 - val_categorical_accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.5007 - mean_absolute_error: 0.5542 - categorical_accuracy: 1.0000 - val_loss: 0.4686 - val_mean_absolute_error: 0.5437 - val_categorical_accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.4990 - mean_absolute_error: 0.5536 - categorical_accuracy: 1.0000 - val_loss: 0.4680 - val_mean_absolute_error: 0.5447 - val_categorical_accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "5847/5847 [==============================] - 0s 71us/step - loss: 0.5031 - mean_absolute_error: 0.5564 - categorical_accuracy: 1.0000 - val_loss: 0.4704 - val_mean_absolute_error: 0.5456 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "5847/5847 [==============================] - 0s 69us/step - loss: 0.5047 - mean_absolute_error: 0.5565 - categorical_accuracy: 1.0000 - val_loss: 0.4660 - val_mean_absolute_error: 0.5446 - val_categorical_accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5015 - mean_absolute_error: 0.5583 - categorical_accuracy: 1.0000 - val_loss: 0.4659 - val_mean_absolute_error: 0.5433 - val_categorical_accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "5847/5847 [==============================] - 1s 94us/step - loss: 0.5022 - mean_absolute_error: 0.5591 - categorical_accuracy: 1.0000 - val_loss: 0.4694 - val_mean_absolute_error: 0.5431 - val_categorical_accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5847/5847 [==============================] - 0s 66us/step - loss: 0.5050 - mean_absolute_error: 0.5593 - categorical_accuracy: 1.0000 - val_loss: 0.4664 - val_mean_absolute_error: 0.5448 - val_categorical_accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "5847/5847 [==============================] - 0s 65us/step - loss: 0.5033 - mean_absolute_error: 0.5567 - categorical_accuracy: 1.0000 - val_loss: 0.4684 - val_mean_absolute_error: 0.5447 - val_categorical_accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "5847/5847 [==============================] - 0s 63us/step - loss: 0.4988 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4700 - val_mean_absolute_error: 0.5450 - val_categorical_accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5009 - mean_absolute_error: 0.5539 - categorical_accuracy: 1.0000 - val_loss: 0.4642 - val_mean_absolute_error: 0.5459 - val_categorical_accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5023 - mean_absolute_error: 0.5544 - categorical_accuracy: 1.0000 - val_loss: 0.4721 - val_mean_absolute_error: 0.5464 - val_categorical_accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 0.4998 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4681 - val_mean_absolute_error: 0.5460 - val_categorical_accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5000 - mean_absolute_error: 0.5550 - categorical_accuracy: 1.0000 - val_loss: 0.4656 - val_mean_absolute_error: 0.5419 - val_categorical_accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.4913 - mean_absolute_error: 0.5503 - categorical_accuracy: 1.0000 - val_loss: 0.4676 - val_mean_absolute_error: 0.5429 - val_categorical_accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.5026 - mean_absolute_error: 0.5559 - categorical_accuracy: 1.0000 - val_loss: 0.4691 - val_mean_absolute_error: 0.5439 - val_categorical_accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 0.5069 - mean_absolute_error: 0.5590 - categorical_accuracy: 1.0000 - val_loss: 0.4659 - val_mean_absolute_error: 0.5412 - val_categorical_accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.4936 - mean_absolute_error: 0.5517 - categorical_accuracy: 1.0000 - val_loss: 0.4679 - val_mean_absolute_error: 0.5438 - val_categorical_accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 0.5012 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4646 - val_mean_absolute_error: 0.5428 - val_categorical_accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5847/5847 [==============================] - 0s 72us/step - loss: 0.4976 - mean_absolute_error: 0.5521 - categorical_accuracy: 1.0000 - val_loss: 0.4662 - val_mean_absolute_error: 0.5420 - val_categorical_accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5847/5847 [==============================] - 0s 73us/step - loss: 0.4976 - mean_absolute_error: 0.5527 - categorical_accuracy: 1.0000 - val_loss: 0.4713 - val_mean_absolute_error: 0.5451 - val_categorical_accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.4996 - mean_absolute_error: 0.5547 - categorical_accuracy: 1.0000 - val_loss: 0.4672 - val_mean_absolute_error: 0.5446 - val_categorical_accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 0.4997 - mean_absolute_error: 0.5556 - categorical_accuracy: 1.0000 - val_loss: 0.4692 - val_mean_absolute_error: 0.5454 - val_categorical_accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.5028 - mean_absolute_error: 0.5580 - categorical_accuracy: 1.0000 - val_loss: 0.4664 - val_mean_absolute_error: 0.5454 - val_categorical_accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5847/5847 [==============================] - 0s 78us/step - loss: 0.5011 - mean_absolute_error: 0.5568 - categorical_accuracy: 1.0000 - val_loss: 0.4681 - val_mean_absolute_error: 0.5445 - val_categorical_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5847/5847 [==============================] - 0s 75us/step - loss: 0.5066 - mean_absolute_error: 0.5579 - categorical_accuracy: 1.0000 - val_loss: 0.4672 - val_mean_absolute_error: 0.5432 - val_categorical_accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5847/5847 [==============================] - 0s 77us/step - loss: 0.4950 - mean_absolute_error: 0.5514 - categorical_accuracy: 1.0000 - val_loss: 0.4658 - val_mean_absolute_error: 0.5415 - val_categorical_accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5847/5847 [==============================] - 0s 81us/step - loss: 0.4982 - mean_absolute_error: 0.5536 - categorical_accuracy: 1.0000 - val_loss: 0.4637 - val_mean_absolute_error: 0.5410 - val_categorical_accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5847/5847 [==============================] - 0s 74us/step - loss: 0.4958 - mean_absolute_error: 0.5513 - categorical_accuracy: 1.0000 - val_loss: 0.4661 - val_mean_absolute_error: 0.5428 - val_categorical_accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5847/5847 [==============================] - 0s 76us/step - loss: 0.4976 - mean_absolute_error: 0.5535 - categorical_accuracy: 1.0000 - val_loss: 0.4638 - val_mean_absolute_error: 0.5404 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c11176eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winemod1.fit(x = X_train_scaled, y = y_train, epochs = 200,verbose=1, batch_size = 64,validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 0s 52us/step\n",
      "\n",
      "Loss = 0.46378947771512546\n",
      "Test Accuracy = 0.5404156184196472\n"
     ]
    }
   ],
   "source": [
    "preds = winemod1.evaluate(x = X_test_scaled, y = y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = winemod1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639    6\n",
      "5140    6\n",
      "5040    5\n",
      "3383    7\n",
      "5176    7\n",
      "4022    6\n",
      "2806    5\n",
      "1779    6\n",
      "1262    5\n",
      "116     6\n",
      "1845    7\n",
      "2519    7\n",
      "1247    5\n",
      "5800    6\n",
      "417     5\n",
      "6402    7\n",
      "1490    6\n",
      "4813    6\n",
      "5149    6\n",
      "4921    7\n",
      "4092    4\n",
      "5088    8\n",
      "1126    6\n",
      "848     5\n",
      "2001    5\n",
      "5875    7\n",
      "2003    6\n",
      "1390    6\n",
      "6123    6\n",
      "4256    7\n",
      "       ..\n",
      "2293    6\n",
      "5955    5\n",
      "1241    5\n",
      "1172    6\n",
      "2380    6\n",
      "5305    6\n",
      "183     5\n",
      "514     7\n",
      "6197    6\n",
      "5217    7\n",
      "4571    6\n",
      "4056    6\n",
      "611     5\n",
      "5650    6\n",
      "3878    5\n",
      "4253    5\n",
      "382     6\n",
      "2557    7\n",
      "91      6\n",
      "566     6\n",
      "761     5\n",
      "426     6\n",
      "1580    6\n",
      "105     5\n",
      "5533    6\n",
      "2347    6\n",
      "2301    4\n",
      "993     5\n",
      "466     6\n",
      "3064    5\n",
      "Name: quality, Length: 650, dtype: int64\n",
      "[[5.7050023]\n",
      " [5.7791557]\n",
      " [6.2936044]\n",
      " [6.336311 ]\n",
      " [6.2314568]\n",
      " [6.2084866]\n",
      " [5.2670527]\n",
      " [5.3392973]\n",
      " [5.379343 ]\n",
      " [5.488753 ]\n",
      " [6.660723 ]\n",
      " [5.9489026]\n",
      " [5.6808877]\n",
      " [6.184843 ]\n",
      " [5.2340612]\n",
      " [6.368513 ]\n",
      " [5.771236 ]\n",
      " [6.430213 ]\n",
      " [5.874786 ]\n",
      " [6.6170664]\n",
      " [5.610387 ]\n",
      " [6.0326953]\n",
      " [6.326288 ]\n",
      " [5.2645526]\n",
      " [5.2776093]\n",
      " [6.7909718]\n",
      " [6.6795044]\n",
      " [5.970762 ]\n",
      " [6.320341 ]\n",
      " [6.386961 ]\n",
      " [5.5560956]\n",
      " [6.333132 ]\n",
      " [6.126674 ]\n",
      " [5.4347057]\n",
      " [5.9391017]\n",
      " [5.5187902]\n",
      " [5.335684 ]\n",
      " [6.6430554]\n",
      " [5.556171 ]\n",
      " [5.6808877]\n",
      " [6.5201583]\n",
      " [5.395166 ]\n",
      " [6.481767 ]\n",
      " [5.7882304]\n",
      " [5.311628 ]\n",
      " [5.698109 ]\n",
      " [6.034071 ]\n",
      " [5.3221893]\n",
      " [5.030727 ]\n",
      " [5.3249383]\n",
      " [5.668904 ]\n",
      " [5.2089233]\n",
      " [5.260907 ]\n",
      " [6.498231 ]\n",
      " [5.2785196]\n",
      " [5.7788906]\n",
      " [5.8474903]\n",
      " [5.8071785]\n",
      " [6.010879 ]\n",
      " [5.83589  ]\n",
      " [6.4079714]\n",
      " [5.2609386]\n",
      " [6.417913 ]\n",
      " [5.338136 ]\n",
      " [5.8214827]\n",
      " [6.2434263]\n",
      " [6.4329166]\n",
      " [5.4723434]\n",
      " [6.236137 ]\n",
      " [5.766279 ]\n",
      " [5.533574 ]\n",
      " [6.139366 ]\n",
      " [5.385204 ]\n",
      " [6.068045 ]\n",
      " [6.2063255]\n",
      " [5.238736 ]\n",
      " [5.546446 ]\n",
      " [5.8503957]\n",
      " [5.465062 ]\n",
      " [5.624821 ]\n",
      " [5.684008 ]\n",
      " [5.168873 ]\n",
      " [6.493844 ]\n",
      " [4.9953394]\n",
      " [5.729517 ]\n",
      " [6.417638 ]\n",
      " [6.127329 ]\n",
      " [6.61294  ]\n",
      " [5.29805  ]\n",
      " [5.295549 ]\n",
      " [5.4884825]\n",
      " [5.822434 ]\n",
      " [6.4118986]\n",
      " [6.4480543]\n",
      " [5.6665993]\n",
      " [5.2714868]\n",
      " [5.4955316]\n",
      " [5.3561683]\n",
      " [5.7665887]\n",
      " [5.3190536]\n",
      " [6.3891582]\n",
      " [5.416    ]\n",
      " [5.277928 ]\n",
      " [6.052885 ]\n",
      " [6.1966934]\n",
      " [5.3817644]\n",
      " [5.513082 ]\n",
      " [5.2419195]\n",
      " [5.3793035]\n",
      " [5.596995 ]\n",
      " [5.6034756]\n",
      " [5.205463 ]\n",
      " [6.4001746]\n",
      " [5.2828026]\n",
      " [6.4272304]\n",
      " [6.212779 ]\n",
      " [6.180628 ]\n",
      " [5.449775 ]\n",
      " [6.4808526]\n",
      " [5.8344   ]\n",
      " [5.4347734]\n",
      " [5.125449 ]\n",
      " [5.148591 ]\n",
      " [5.324167 ]\n",
      " [5.342264 ]\n",
      " [5.2808666]\n",
      " [5.2726364]\n",
      " [5.3843846]\n",
      " [6.663915 ]\n",
      " [5.333479 ]\n",
      " [5.0492487]\n",
      " [5.295838 ]\n",
      " [5.658304 ]\n",
      " [6.5432396]\n",
      " [5.3677454]\n",
      " [6.0722895]\n",
      " [5.6804457]\n",
      " [5.319394 ]\n",
      " [5.976382 ]\n",
      " [5.5428963]\n",
      " [6.046636 ]\n",
      " [5.4300933]\n",
      " [5.2585897]\n",
      " [5.32208  ]\n",
      " [5.327374 ]\n",
      " [6.198131 ]\n",
      " [5.520899 ]\n",
      " [5.4879   ]\n",
      " [6.239641 ]\n",
      " [5.296901 ]\n",
      " [5.7965364]\n",
      " [5.6092834]\n",
      " [5.0955315]\n",
      " [5.1962295]\n",
      " [6.246317 ]\n",
      " [5.632557 ]\n",
      " [6.267915 ]\n",
      " [6.767339 ]\n",
      " [5.4429164]\n",
      " [5.248518 ]\n",
      " [5.7522964]\n",
      " [5.281242 ]\n",
      " [6.494339 ]\n",
      " [5.3363686]\n",
      " [6.199791 ]\n",
      " [5.817164 ]\n",
      " [5.578892 ]\n",
      " [5.9880776]\n",
      " [5.487044 ]\n",
      " [5.6689463]\n",
      " [6.193737 ]\n",
      " [5.258074 ]\n",
      " [5.289346 ]\n",
      " [5.2433934]\n",
      " [6.2364955]\n",
      " [6.3459177]\n",
      " [5.236883 ]\n",
      " [6.291673 ]\n",
      " [6.6634517]\n",
      " [5.976664 ]\n",
      " [5.6414533]\n",
      " [6.5400505]\n",
      " [6.4512978]\n",
      " [5.9420404]\n",
      " [5.9594235]\n",
      " [5.5871353]\n",
      " [6.09429  ]\n",
      " [5.544911 ]\n",
      " [5.688982 ]\n",
      " [6.7053246]\n",
      " [5.262993 ]\n",
      " [6.3581705]\n",
      " [6.4342337]\n",
      " [5.3716865]\n",
      " [4.985137 ]\n",
      " [5.7331514]\n",
      " [5.321364 ]\n",
      " [5.546542 ]\n",
      " [5.581245 ]\n",
      " [5.778518 ]\n",
      " [5.3605795]\n",
      " [5.4254537]\n",
      " [5.3191986]\n",
      " [6.464035 ]\n",
      " [5.2646008]\n",
      " [5.226479 ]\n",
      " [5.648126 ]\n",
      " [5.7134786]\n",
      " [6.504012 ]\n",
      " [5.689409 ]\n",
      " [6.660351 ]\n",
      " [6.3550434]\n",
      " [5.221828 ]\n",
      " [5.6854024]\n",
      " [5.5412226]\n",
      " [6.4927254]\n",
      " [5.8254247]\n",
      " [5.398282 ]\n",
      " [6.6435046]\n",
      " [5.2020926]\n",
      " [5.3734713]\n",
      " [6.1836953]\n",
      " [5.6265683]\n",
      " [6.5920906]\n",
      " [5.898445 ]\n",
      " [5.934396 ]\n",
      " [5.759765 ]\n",
      " [5.281836 ]\n",
      " [6.7370567]\n",
      " [5.2552247]\n",
      " [6.466707 ]\n",
      " [6.363247 ]\n",
      " [6.7988043]\n",
      " [5.6482167]\n",
      " [6.606351 ]\n",
      " [5.4007483]\n",
      " [6.252306 ]\n",
      " [5.902045 ]\n",
      " [6.314034 ]\n",
      " [5.2881002]\n",
      " [6.3574133]\n",
      " [5.501648 ]\n",
      " [6.4930735]\n",
      " [6.0335097]\n",
      " [6.199141 ]\n",
      " [5.2304287]\n",
      " [5.750015 ]\n",
      " [5.3676057]\n",
      " [5.119485 ]\n",
      " [5.940293 ]\n",
      " [5.7745466]\n",
      " [6.7641087]\n",
      " [5.6279454]\n",
      " [6.551257 ]\n",
      " [6.4889393]\n",
      " [6.289148 ]\n",
      " [5.9039493]\n",
      " [5.5740175]\n",
      " [5.69395  ]\n",
      " [5.705519 ]\n",
      " [5.237548 ]\n",
      " [5.2579327]\n",
      " [5.417595 ]\n",
      " [5.232601 ]\n",
      " [6.0888023]\n",
      " [5.2972546]\n",
      " [5.5111246]\n",
      " [6.395547 ]\n",
      " [5.6336107]\n",
      " [5.262993 ]\n",
      " [5.1564445]\n",
      " [6.4136844]\n",
      " [6.1329   ]\n",
      " [6.3311024]\n",
      " [5.28208  ]\n",
      " [5.260735 ]\n",
      " [5.3086596]\n",
      " [5.795611 ]\n",
      " [6.472509 ]\n",
      " [5.4388804]\n",
      " [6.327549 ]\n",
      " [6.299917 ]\n",
      " [5.618039 ]\n",
      " [5.6210947]\n",
      " [5.233061 ]\n",
      " [5.4554176]\n",
      " [5.66449  ]\n",
      " [6.2704763]\n",
      " [5.3380156]\n",
      " [5.3880835]\n",
      " [5.346624 ]\n",
      " [5.2643247]\n",
      " [5.5736623]\n",
      " [6.44484  ]\n",
      " [5.658304 ]\n",
      " [6.3710365]\n",
      " [5.3252506]\n",
      " [6.2296414]\n",
      " [5.5700865]\n",
      " [6.7148914]\n",
      " [6.6177073]\n",
      " [5.441483 ]\n",
      " [5.261089 ]\n",
      " [5.4121947]\n",
      " [5.305518 ]\n",
      " [5.43776  ]\n",
      " [5.34397  ]\n",
      " [6.5765142]\n",
      " [5.305362 ]\n",
      " [5.2917914]\n",
      " [5.928422 ]\n",
      " [5.107192 ]\n",
      " [5.4560337]\n",
      " [4.969631 ]\n",
      " [6.3117905]\n",
      " [6.294236 ]\n",
      " [5.338622 ]\n",
      " [5.752253 ]\n",
      " [5.674322 ]\n",
      " [5.1685944]\n",
      " [5.7821236]\n",
      " [5.9660425]\n",
      " [5.5866404]\n",
      " [5.290345 ]\n",
      " [5.233155 ]\n",
      " [6.119302 ]\n",
      " [5.997365 ]\n",
      " [6.122063 ]\n",
      " [6.420557 ]\n",
      " [6.6281195]\n",
      " [5.435343 ]\n",
      " [6.146228 ]\n",
      " [5.6135697]\n",
      " [5.4649973]\n",
      " [6.34884  ]\n",
      " [6.4428244]\n",
      " [6.4922395]\n",
      " [5.8157396]\n",
      " [5.633111 ]\n",
      " [6.5042467]\n",
      " [5.418336 ]\n",
      " [6.4341764]\n",
      " [5.1034517]\n",
      " [5.5472713]\n",
      " [5.299908 ]\n",
      " [7.013997 ]\n",
      " [5.1719193]\n",
      " [6.5129604]\n",
      " [5.5397687]\n",
      " [6.1232953]\n",
      " [5.916049 ]\n",
      " [5.279627 ]\n",
      " [6.539141 ]\n",
      " [6.400111 ]\n",
      " [6.1937985]\n",
      " [5.7106442]\n",
      " [6.3636055]\n",
      " [6.7316637]\n",
      " [5.630554 ]\n",
      " [6.3508854]\n",
      " [5.3692327]\n",
      " [6.3685894]\n",
      " [5.465515 ]\n",
      " [5.404155 ]\n",
      " [5.661284 ]\n",
      " [5.353177 ]\n",
      " [6.1574855]\n",
      " [5.292948 ]\n",
      " [5.7067213]\n",
      " [6.5583386]\n",
      " [5.7269816]\n",
      " [5.339564 ]\n",
      " [5.852396 ]\n",
      " [5.287719 ]\n",
      " [6.0688286]\n",
      " [5.298404 ]\n",
      " [4.8598013]\n",
      " [5.43228  ]\n",
      " [5.827137 ]\n",
      " [5.145061 ]\n",
      " [5.8251376]\n",
      " [5.2611017]\n",
      " [5.2272925]\n",
      " [4.9634423]\n",
      " [6.177579 ]\n",
      " [5.6200433]\n",
      " [5.7571   ]\n",
      " [6.6169996]\n",
      " [6.7370567]\n",
      " [5.389374 ]\n",
      " [6.296727 ]\n",
      " [6.158189 ]\n",
      " [5.9085584]\n",
      " [5.364587 ]\n",
      " [6.312929 ]\n",
      " [6.004689 ]\n",
      " [5.325691 ]\n",
      " [5.4037232]\n",
      " [6.5095167]\n",
      " [6.091994 ]\n",
      " [5.324668 ]\n",
      " [5.7505903]\n",
      " [6.1673117]\n",
      " [5.334814 ]\n",
      " [5.632203 ]\n",
      " [5.1844177]\n",
      " [5.392208 ]\n",
      " [5.951033 ]\n",
      " [5.0745277]\n",
      " [5.365823 ]\n",
      " [5.2070937]\n",
      " [6.572076 ]\n",
      " [5.9472075]\n",
      " [5.7359548]\n",
      " [6.236137 ]\n",
      " [6.6281195]\n",
      " [6.2387667]\n",
      " [5.3553333]\n",
      " [5.995357 ]\n",
      " [5.970134 ]\n",
      " [5.3468604]\n",
      " [5.1324306]\n",
      " [6.4432993]\n",
      " [6.450309 ]\n",
      " [5.1948504]\n",
      " [5.1819825]\n",
      " [5.321181 ]\n",
      " [5.7851834]\n",
      " [5.6768284]\n",
      " [6.369282 ]\n",
      " [6.1021876]\n",
      " [6.293922 ]\n",
      " [5.885076 ]\n",
      " [5.293881 ]\n",
      " [6.5747175]\n",
      " [5.5683293]\n",
      " [5.3518   ]\n",
      " [5.9998803]\n",
      " [5.307735 ]\n",
      " [5.3012953]\n",
      " [5.6694217]\n",
      " [6.6159935]\n",
      " [6.546925 ]\n",
      " [6.2172594]\n",
      " [6.0971847]\n",
      " [5.543798 ]\n",
      " [5.3238626]\n",
      " [6.1737814]\n",
      " [5.8051805]\n",
      " [6.419053 ]\n",
      " [5.570878 ]\n",
      " [5.1819825]\n",
      " [6.131556 ]\n",
      " [6.127357 ]\n",
      " [5.8186407]\n",
      " [5.661534 ]\n",
      " [5.718347 ]\n",
      " [5.9971957]\n",
      " [5.248646 ]\n",
      " [5.923672 ]\n",
      " [5.3203936]\n",
      " [5.595839 ]\n",
      " [5.051799 ]\n",
      " [5.6945014]\n",
      " [5.2532406]\n",
      " [5.910809 ]\n",
      " [6.0730467]\n",
      " [5.35755  ]\n",
      " [6.036225 ]\n",
      " [6.619496 ]\n",
      " [6.5963836]\n",
      " [6.1707196]\n",
      " [5.3080506]\n",
      " [6.571667 ]\n",
      " [6.0727925]\n",
      " [6.778822 ]\n",
      " [5.4699264]\n",
      " [5.4603186]\n",
      " [6.484377 ]\n",
      " [5.9388666]\n",
      " [6.26366  ]\n",
      " [5.7531605]\n",
      " [5.8214827]\n",
      " [5.2041025]\n",
      " [6.4781895]\n",
      " [6.3196616]\n",
      " [5.437384 ]\n",
      " [5.330637 ]\n",
      " [5.734019 ]\n",
      " [5.600251 ]\n",
      " [5.179258 ]\n",
      " [5.389795 ]\n",
      " [5.8214545]\n",
      " [5.806522 ]\n",
      " [5.8120294]\n",
      " [6.7256727]\n",
      " [6.2556086]\n",
      " [6.33874  ]\n",
      " [5.2754726]\n",
      " [6.377766 ]\n",
      " [6.516195 ]\n",
      " [5.6843276]\n",
      " [6.2041593]\n",
      " [5.811691 ]\n",
      " [5.5031466]\n",
      " [5.5630026]\n",
      " [5.052683 ]\n",
      " [5.416197 ]\n",
      " [5.385088 ]\n",
      " [6.0391545]\n",
      " [5.926036 ]\n",
      " [5.6512146]\n",
      " [4.9886904]\n",
      " [5.330605 ]\n",
      " [5.342629 ]\n",
      " [5.133192 ]\n",
      " [5.7652893]\n",
      " [6.1642694]\n",
      " [6.2497854]\n",
      " [6.6616473]\n",
      " [5.592725 ]\n",
      " [6.290016 ]\n",
      " [5.129121 ]\n",
      " [6.3437347]\n",
      " [6.177132 ]\n",
      " [6.3234925]\n",
      " [6.559732 ]\n",
      " [5.894463 ]\n",
      " [5.509404 ]\n",
      " [6.458647 ]\n",
      " [5.2900605]\n",
      " [5.8718114]\n",
      " [6.3574686]\n",
      " [6.314478 ]\n",
      " [5.9203353]\n",
      " [6.8861036]\n",
      " [6.576423 ]\n",
      " [5.8036404]\n",
      " [5.294789 ]\n",
      " [6.1759553]\n",
      " [6.886808 ]\n",
      " [6.301931 ]\n",
      " [5.9060006]\n",
      " [5.524707 ]\n",
      " [6.4471374]\n",
      " [5.381691 ]\n",
      " [6.4934554]\n",
      " [5.515023 ]\n",
      " [6.32872  ]\n",
      " [5.698271 ]\n",
      " [5.8582296]\n",
      " [5.2675695]\n",
      " [5.9611406]\n",
      " [5.3597093]\n",
      " [6.481539 ]\n",
      " [6.8213325]\n",
      " [6.258836 ]\n",
      " [6.2827272]\n",
      " [5.43028  ]\n",
      " [6.3368044]\n",
      " [6.2295356]\n",
      " [6.6413198]\n",
      " [6.2521024]\n",
      " [6.3535504]\n",
      " [5.3080506]\n",
      " [5.9305444]\n",
      " [6.187452 ]\n",
      " [6.0850453]\n",
      " [6.4188056]\n",
      " [6.504442 ]\n",
      " [5.9091063]\n",
      " [5.241241 ]\n",
      " [5.8624625]\n",
      " [6.6216164]\n",
      " [6.294275 ]\n",
      " [5.767373 ]\n",
      " [5.9385896]\n",
      " [5.911671 ]\n",
      " [5.097897 ]\n",
      " [5.3390746]\n",
      " [5.432012 ]\n",
      " [6.5555325]\n",
      " [5.118216 ]\n",
      " [5.2933083]\n",
      " [6.3234925]\n",
      " [5.6084375]\n",
      " [6.014969 ]\n",
      " [6.2821374]\n",
      " [5.20292  ]\n",
      " [5.3086596]\n",
      " [6.338584 ]\n",
      " [5.834032 ]\n",
      " [6.360056 ]\n",
      " [5.252958 ]\n",
      " [5.4226804]\n",
      " [6.7500896]\n",
      " [6.352673 ]\n",
      " [5.809137 ]\n",
      " [6.113587 ]\n",
      " [6.4438424]\n",
      " [5.0103755]\n",
      " [5.6124425]\n",
      " [6.246277 ]\n",
      " [5.597183 ]\n",
      " [6.223664 ]\n",
      " [6.0685143]\n",
      " [5.9585085]\n",
      " [6.2856293]\n",
      " [5.257658 ]\n",
      " [5.65865  ]\n",
      " [6.4685287]\n",
      " [5.84638  ]\n",
      " [6.538033 ]\n",
      " [6.0318575]\n",
      " [5.7154202]\n",
      " [6.190518 ]\n",
      " [4.537348 ]\n",
      " [5.971698 ]\n",
      " [5.6299996]\n",
      " [6.488873 ]\n",
      " [5.9771957]\n",
      " [5.5925627]\n",
      " [5.474844 ]\n",
      " [6.4879475]\n",
      " [5.292253 ]\n",
      " [6.204581 ]\n",
      " [5.1438794]\n",
      " [6.3411474]\n",
      " [5.7308693]\n",
      " [5.726586 ]\n",
      " [6.046085 ]\n",
      " [5.8789425]\n",
      " [5.3559055]\n",
      " [6.3855205]\n",
      " [5.128468 ]\n",
      " [5.1467752]\n",
      " [5.755165 ]\n",
      " [6.1739664]\n",
      " [5.4571557]\n",
      " [5.456632 ]\n",
      " [5.141418 ]\n",
      " [5.5915794]\n",
      " [6.1919546]\n",
      " [5.1685944]\n",
      " [6.1030283]\n",
      " [5.894463 ]\n",
      " [5.341957 ]\n",
      " [5.4605207]\n",
      " [6.3426313]\n",
      " [5.992484 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
